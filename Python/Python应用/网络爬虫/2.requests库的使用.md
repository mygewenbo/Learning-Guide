# requests库的使用

## 一、requests库简介

requests是Python中用于发送HTTP请求的第三方库，它提供了简洁、易用的API，使得发送HTTP请求变得非常简单。requests库基于urllib3，但比urllib3更加人性化，是Python爬虫开发中最常用的HTTP库之一。

### requests库的主要特点
- 简洁易用的API设计
- 支持HTTP/1.1和HTTP/2
- 支持多种HTTP方法（GET、POST、PUT、DELETE等）
- 支持会话管理和Cookie持久化
- 支持文件上传和下载
- 支持自动解压缩（gzip、deflate）
- 支持自动重定向
- 支持SSL证书验证
- 支持代理设置
- 支持超时设置
- 支持请求和响应的内容编码处理

## 二、requests库的安装

### 使用pip安装
```bash
pip install requests
```

### 验证安装
```python
import requests
print(requests.__version__)
```

## 三、requests库的基本用法

### 1. 发送GET请求

GET请求是最常用的HTTP请求方法，用于获取资源。

#### 基本GET请求
```python
import requests

# 发送GET请求
response = requests.get('https://httpbin.org/get')

# 打印响应状态码
print('状态码:', response.status_code)

# 打印响应头
print('响应头:', response.headers)

# 打印响应内容（字符串形式）
print('响应内容:', response.text)

# 打印响应内容（字节形式）
print('响应内容（字节）:', response.content)

# 打印响应内容（JSON格式）
print('响应内容（JSON）:', response.json())
```

#### GET请求带参数
```python
import requests

# 方法1：直接在URL中添加参数
response = requests.get('https://httpbin.org/get?name=zhangsan&age=20')
print(response.json())

# 方法2：使用params参数
params = {
    'name': 'zhangsan',
    'age': 20
}
response = requests.get('https://httpbin.org/get', params=params)
print(response.json())
```

### 2. 发送POST请求

POST请求用于向服务器提交数据，常用于表单提交、文件上传等。

#### 基本POST请求
```python
import requests

# 发送POST请求，提交表单数据
data = {
    'name': 'zhangsan',
    'age': 20
}
response = requests.post('https://httpbin.org/post', data=data)
print(response.json())
```

#### POST请求发送JSON数据
```python
import requests
import json

# 方法1：使用json参数
json_data = {
    'name': 'zhangsan',
    'age': 20
}
response = requests.post('https://httpbin.org/post', json=json_data)
print(response.json())

# 方法2：使用data参数和headers
json_data = json.dumps({
    'name': 'zhangsan',
    'age': 20
})
headers = {
    'Content-Type': 'application/json'
}
response = requests.post('https://httpbin.org/post', data=json_data, headers=headers)
print(response.json())
```

### 3. 发送其他HTTP方法请求

requests库支持所有HTTP方法：

```python
import requests

# PUT请求
response = requests.put('https://httpbin.org/put', data={'name': 'zhangsan'})
print(response.json())

# DELETE请求
response = requests.delete('https://httpbin.org/delete')
print(response.json())

# HEAD请求
response = requests.head('https://httpbin.org/get')
print(response.headers)

# OPTIONS请求
response = requests.options('https://httpbin.org/get')
print(response.headers['Allow'])
```

## 四、requests库的高级用法

### 1. 自定义请求头

在发送请求时，可以自定义请求头，用于模拟浏览器、设置认证信息等。

```python
import requests

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
    'Accept-Language': 'en-US,en;q=0.5',
    'Referer': 'https://www.example.com/'
}

response = requests.get('https://httpbin.org/get', headers=headers)
print(response.json())
```

### 2. 处理Cookie

requests库自动处理Cookie，可以通过cookies参数发送Cookie，也可以通过response.cookies获取响应中的Cookie。

#### 发送Cookie
```python
import requests

# 方法1：使用cookies参数
cookies = {
    'name': 'zhangsan',
    'age': '20'
}
response = requests.get('https://httpbin.org/cookies', cookies=cookies)
print(response.json())

# 方法2：使用CookieJar对象
from http.cookiejar import CookieJar
from http.cookies import SimpleCookie

cookie_jar = CookieJar()
cookie_string = 'name=zhangsan; age=20'
simple_cookie = SimpleCookie(cookie_string)

for key, morsel in simple_cookie.items():
    cookie = requests.cookies.create_cookie(
        name=key,
        value=morsel.value,
        domain='httpbin.org',
        path='/'
    )
    cookie_jar.set_cookie(cookie)

response = requests.get('https://httpbin.org/cookies', cookies=cookie_jar)
print(response.json())
```

#### 获取Cookie
```python
import requests

response = requests.get('https://httpbin.org/cookies/set?name=zhangsan&age=20')
print('响应中的Cookie:', response.cookies)
print('Cookie值:', response.cookies.get('name'))
```

### 3. 会话管理

使用Session对象可以保持会话状态，自动处理Cookie，适合需要登录后访问多个页面的场景。

```python
import requests

# 创建Session对象
session = requests.Session()

# 发送登录请求（假设需要登录）
login_data = {
    'username': 'admin',
    'password': '123456'
}
session.post('https://httpbin.org/post', data=login_data)

# 使用同一个Session发送其他请求，会自动携带Cookie
response = session.get('https://httpbin.org/cookies')
print(response.json())

# 关闭Session
session.close()
```

### 4. 处理超时

设置超时时间可以防止请求无限期等待，提高程序的健壮性。

```python
import requests

# 设置超时时间（连接超时和读取超时）
try:
    response = requests.get('https://httpbin.org/delay/3', timeout=(5, 10))
    print(response.status_code)
except requests.exceptions.Timeout as e:
    print('请求超时:', e)
except requests.exceptions.RequestException as e:
    print('请求异常:', e)
```

### 5. 处理代理

使用代理可以隐藏真实IP地址，绕过IP限制等。

```python
import requests

# 单个代理
proxies = {
    'http': 'http://127.0.0.1:8080',
    'https': 'https://127.0.0.1:8080'
}

# 带认证的代理
proxies = {
    'http': 'http://username:password@127.0.0.1:8080',
    'https': 'https://username:password@127.0.0.1:8080'
}

response = requests.get('https://httpbin.org/ip', proxies=proxies)
print(response.json())
```

### 6. 处理SSL证书验证

requests库默认验证SSL证书，可以通过verify参数控制。

```python
import requests

# 验证SSL证书（默认）
response = requests.get('https://httpbin.org/get', verify=True)
print(response.status_code)

# 不验证SSL证书
response = requests.get('https://httpbin.org/get', verify=False)
print(response.status_code)

# 指定CA证书
response = requests.get('https://httpbin.org/get', verify='/path/to/cert.pem')
print(response.status_code)
```

### 7. 文件上传

requests库支持文件上传功能。

```python
import requests

# 上传单个文件
files = {
    'file': open('example.txt', 'rb')
}
response = requests.post('https://httpbin.org/post', files=files)
print(response.json())

# 上传多个文件
files = [
    ('files', ('example1.txt', open('example1.txt', 'rb'), 'text/plain')),
    ('files', ('example2.txt', open('example2.txt', 'rb'), 'text/plain'))
]
response = requests.post('https://httpbin.org/post', files=files)
print(response.json())

# 上传文件并带其他数据
data = {'name': 'zhangsan'}
files = {'file': open('example.txt', 'rb')}
response = requests.post('https://httpbin.org/post', data=data, files=files)
print(response.json())
```

### 8. 流式请求

对于大文件或长时间运行的响应，可以使用流式请求，边接收边处理数据。

```python
import requests

# 流式请求
response = requests.get('https://httpbin.org/stream/10', stream=True)

# 逐行处理响应内容
for line in response.iter_lines():
    if line:
        print(line.decode('utf-8'))

# 逐块处理响应内容
for chunk in response.iter_content(chunk_size=128):
    print(chunk)

# 关闭响应
response.close()
```

### 9. 下载文件

使用requests库可以方便地下载文件。

```python
import requests

# 下载小文件
response = requests.get('https://httpbin.org/image/jpeg')
with open('image.jpg', 'wb') as f:
    f.write(response.content)

# 下载大文件（流式下载）
response = requests.get('https://httpbin.org/image/jpeg', stream=True)
with open('image.jpg', 'wb') as f:
    for chunk in response.iter_content(chunk_size=1024):
        if chunk:
            f.write(chunk)
```

### 10. 处理重定向

requests库默认自动处理重定向，可以通过allow_redirects参数控制。

```python
import requests

# 自动处理重定向（默认）
response = requests.get('https://httpbin.org/redirect/1')
print('是否重定向:', response.history)
print('最终URL:', response.url)

# 不处理重定向
response = requests.get('https://httpbin.org/redirect/1', allow_redirects=False)
print('状态码:', response.status_code)
print('是否重定向:', response.history)
print('当前URL:', response.url)
```

## 五、requests库的响应对象

当发送请求后，requests库返回一个Response对象，包含了响应的所有信息。

### Response对象的主要属性

| 属性 | 描述 |
|------|------|
| `status_code` | HTTP状态码 |
| `headers` | 响应头，字典格式 |
| `cookies` | 响应中的Cookie，RequestsCookieJar对象 |
| `url` | 响应的URL |
| `history` | 重定向历史，包含之前的Response对象 |
| `encoding` | 响应内容的编码 |
| `content` | 响应内容的字节形式 |
| `text` | 响应内容的字符串形式，自动根据encoding解码 |
| `json()` | 将响应内容解析为JSON格式（如果可能） |
| `elapsed` | 请求耗时，datetime.timedelta对象 |
| `reason` | 状态码的文字描述 |
| `ok` | 如果状态码小于400，返回True，否则返回False |
| `is_redirect` | 如果是重定向响应，返回True |
| `is_permanent_redirect` | 如果是永久重定向响应，返回True |

### Response对象的主要方法

| 方法 | 描述 |
|------|------|
| `json()` | 将响应内容解析为JSON格式 |
| `iter_content(chunk_size=1)` | 迭代响应内容，适合处理大文件 |
| `iter_lines(chunk_size=512, decode_unicode=None, delimiter=None)` | 按行迭代响应内容 |
| `close()` | 关闭底层的TCP连接 |
| `raise_for_status()` | 如果状态码大于等于400，抛出HTTPError异常 |

## 六、requests库的异常处理

requests库定义了多种异常，用于处理不同类型的错误。

### 主要异常类型

| 异常类型 | 描述 |
|----------|------|
| `RequestException` | 所有requests异常的基类 |
| `HTTPError` | HTTP错误，状态码大于等于400 |
| `ConnectionError` | 连接错误，如DNS解析失败、连接拒绝等 |
| `Timeout` | 请求超时 |
| `TooManyRedirects` | 重定向次数超过限制 |
| `URLRequired` | URL格式错误 |
| `InvalidURL` | URL无效 |
| `InvalidSchema` | 无效的协议方案 |
| `InvalidHeader` | 无效的请求头 |
| `ChunkedEncodingError` | 分块编码错误 |
| `ContentDecodingError` | 内容解码错误 |
| `StreamConsumedError` | 流式响应已被消费 |

### 异常处理示例

```python
import requests
from requests.exceptions import RequestException, HTTPError, ConnectionError, Timeout

try:
    response = requests.get('https://httpbin.org/status/404')
    # 如果状态码大于等于400，抛出HTTPError异常
    response.raise_for_status()
    print('请求成功')
except HTTPError as e:
    print('HTTP错误:', e)
except ConnectionError as e:
    print('连接错误:', e)
except Timeout as e:
    print('请求超时:', e)
except RequestException as e:
    print('请求异常:', e)
```

## 七、requests库的高级功能

### 1. 事件钩子

使用事件钩子可以在请求的不同阶段执行自定义函数。

```python
import requests

# 定义响应钩子函数
def response_hook(response, *args, **kwargs):
    print('响应状态码:', response.status_code)
    response.hook_called = True
    return response

# 发送请求时使用钩子
response = requests.get('https://httpbin.org/get', hooks={'response': response_hook})
print('钩子是否被调用:', getattr(response, 'hook_called', False))
```

### 2. 自定义认证

requests库支持自定义认证方式，用于处理特殊的认证需求。

```python
import requests
from requests.auth import AuthBase

# 自定义认证类
class MyAuth(AuthBase):
    def __init__(self, username, password):
        self.username = username
        self.password = password
    
    def __call__(self, r):
        # 在请求头中添加认证信息
        r.headers['X-Auth-Username'] = self.username
        r.headers['X-Auth-Password'] = self.password
        return r

# 使用自定义认证
response = requests.get('https://httpbin.org/get', auth=MyAuth('admin', '123456'))
print(response.json())
```

### 3. 会话级别的配置

可以为Session对象设置默认配置，如请求头、超时时间等。

```python
import requests

# 创建Session对象
session = requests.Session()

# 设置默认请求头
session.headers.update({
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
})

# 设置默认超时时间
session.timeout = (5, 10)

# 发送请求，会使用默认配置
response = session.get('https://httpbin.org/get')
print(response.json())

# 关闭Session
session.close()
```

## 八、requests库的最佳实践

### 1. 错误处理
- 始终处理可能的异常，尤其是网络请求
- 使用`raise_for_status()`检查HTTP错误
- 区分不同类型的异常，采取不同的处理策略

### 2. 资源管理
- 使用`with`语句管理Session对象，确保资源被正确释放
- 对于流式请求，记得关闭响应
- 及时释放不再使用的资源

### 3. 性能优化
- 对于大文件，使用流式请求和流式下载
- 合理设置超时时间
- 使用连接池（Session对象自动使用连接池）
- 避免重复创建Session对象

### 4. 安全性
- 验证SSL证书，避免中间人攻击
- 不要在代码中硬编码敏感信息（如密码、API密钥）
- 使用HTTPS协议
- 合理设置Cookie的属性（如Secure、HttpOnly）

### 5. 爬虫伦理
- 遵守网站的robots.txt规则
- 合理设置请求间隔，避免对服务器造成过大压力
- 不要过度爬取网站数据
- 尊重网站的版权和知识产权

## 九、requests库的扩展

requests库有很多扩展库，可以增强其功能：

- `requests-cache`：为requests添加缓存功能
- `requests-oauthlib`：OAuth认证支持
- `requests-toolbelt`：提供额外的工具，如多部分表单数据、SSL证书验证等
- `requests-html`：HTML解析和渲染支持

### 安装扩展库示例
```bash
pip install requests-cache requests-oauthlib requests-toolbelt requests-html
```

## 十、练习题

1. 使用requests库发送GET请求，获取https://httpbin.org/get的响应，并打印响应状态码、响应头和响应内容。
2. 使用requests库发送POST请求，向https://httpbin.org/post提交表单数据，并打印响应内容。
3. 使用requests库发送POST请求，向https://httpbin.org/post提交JSON数据，并打印响应内容。
4. 使用Session对象模拟登录，然后访问其他页面。
5. 设置超时时间，处理请求超时异常。
6. 使用代理发送请求，隐藏真实IP。
7. 下载一个图片文件，并保存到本地。
8. 处理不同类型的HTTP状态码，如200、404、500等。
9. 使用事件钩子，在请求完成后执行自定义函数。
10. 实现一个简单的爬虫，爬取某个网站的标题和链接。

## 十一、参考资料

- [requests官方文档](https://docs.python-requests.org/)
- [requests GitHub仓库](https://github.com/psf/requests)
- [HTTP协议基础](https://developer.mozilla.org/en-US/docs/Web/HTTP)
- [Python爬虫开发实战](https://book.douban.com/subject/30365809/)

## 十二、总结

requests库是Python中最常用的HTTP库之一，它提供了简洁、易用的API，使得发送HTTP请求变得非常简单。本教程详细介绍了requests库的安装、基本用法、高级功能、异常处理和最佳实践，涵盖了requests库的主要特性和使用方法。

通过学习本教程，你应该能够熟练使用requests库发送各种HTTP请求，处理响应数据，处理异常情况，并遵循最佳实践编写高质量的爬虫代码。在后续的学习中，我们将学习如何使用BeautifulSoup和lxml解析HTML，以及如何使用Scrapy框架开发更复杂的爬虫。