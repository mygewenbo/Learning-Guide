# ç¬¬ä¸€é˜¶æ®µï¼šLangChain åŸºç¡€å…¥é—¨ï¼ˆè¯¦ç»†è®²è§£ï¼‰

> **å­¦ä¹ ç›®æ ‡**ï¼šç†è§£ LangChain æ ¸å¿ƒæ¦‚å¿µã€æŒæ¡æ¨¡å‹åˆå§‹åŒ–å’Œè°ƒç”¨ã€å­¦ä¼šå®šä¹‰å·¥å…·ã€åˆ›å»ºç®€å•çš„ Agent
> 
> **é¢„è®¡å­¦ä¹ æ—¶é—´**ï¼š1-2å‘¨
> 
> **å‰ç½®è¦æ±‚**ï¼šPython åŸºç¡€ã€äº†è§£ API è°ƒç”¨ã€åŸºæœ¬çš„å¼‚æ­¥ç¼–ç¨‹æ¦‚å¿µ

---

## ğŸ“š ç¬¬ä¸€é˜¶æ®µå­¦ä¹ å¤§çº²

1. [LangChain æ¦‚è¿°](#ç¬¬ä¸€éƒ¨åˆ†langchain-æ¦‚è¿°)
   - ä»€ä¹ˆæ˜¯ LangChainï¼Ÿ
   - ä¸ºä»€ä¹ˆéœ€è¦ LangChainï¼Ÿ
   - LangChain çš„æ ¸å¿ƒä»·å€¼
   - åº”ç”¨åœºæ™¯è¯¦è§£
   
2. [å®‰è£…ä¸ç¯å¢ƒé…ç½®](#ç¬¬äºŒéƒ¨åˆ†å®‰è£…ä¸ç¯å¢ƒé…ç½®)
   - åŸºç¡€å®‰è£…
   - å„æ¨¡å‹æä¾›å•†é›†æˆ
   - ç¯å¢ƒå˜é‡é…ç½®
   - éªŒè¯å®‰è£…

3. [æ ¸å¿ƒæ¶æ„æ·±å…¥ç†è§£](#ç¬¬ä¸‰éƒ¨åˆ†æ ¸å¿ƒæ¶æ„æ·±å…¥ç†è§£)
   - Runnable æ¥å£è¯¦è§£
   - LCEL è¡¨è¾¾å¼è¯­è¨€
   - é“¾å¼è°ƒç”¨æœºåˆ¶
   - ç»„ä»¶åŒ–è®¾è®¡æ€æƒ³

4. [æ¨¡å‹é›†æˆå®Œå…¨æŒ‡å—](#ç¬¬å››éƒ¨åˆ†æ¨¡å‹é›†æˆå®Œå…¨æŒ‡å—)
   - åˆå§‹åŒ–æ¨¡å‹çš„ä¸‰ç§æ–¹å¼
   - å„ä¸»æµæ¨¡å‹æä¾›å•†é…ç½®
   - æ¨¡å‹è°ƒç”¨è¯¦è§£
   - å‚æ•°è°ƒä¼˜

5. [åˆ›å»ºç¬¬ä¸€ä¸ª Agent](#ç¬¬äº”éƒ¨åˆ†åˆ›å»ºç¬¬ä¸€ä¸ª-agent)
   - Agent åŸºæœ¬æ¦‚å¿µ
   - ç®€å• Agent åˆ›å»º
   - Agent æ‰§è¡Œæµç¨‹
   - å®æˆ˜æ¡ˆä¾‹

---

## ç¬¬ä¸€éƒ¨åˆ†ï¼šLangChain æ¦‚è¿°

### 1.1 ä»€ä¹ˆæ˜¯ LangChainï¼Ÿ

**LangChain** æ˜¯ä¸€ä¸ªä¸“ä¸ºå¼€å‘ **å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é©±åŠ¨çš„åº”ç”¨ç¨‹åº** è€Œè®¾è®¡çš„æ¡†æ¶ã€‚å®ƒä¸æ˜¯ä¸€ä¸ª LLMï¼Œè€Œæ˜¯ä¸€ä¸ª**ç¼–æ’æ¡†æ¶**ï¼Œå¸®åŠ©å¼€å‘è€…å°† LLM ä¸å…¶ä»–å·¥å…·ã€æ•°æ®æºå’ŒæœåŠ¡é›†æˆåœ¨ä¸€èµ·ã€‚

#### æ ¸å¿ƒç†å¿µ

```
ä¼ ç»Ÿåº”ç”¨              LLM åº”ç”¨ï¼ˆä½¿ç”¨ LangChainï¼‰
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€         â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ç”¨æˆ·è¾“å…¥              ç”¨æˆ·è¾“å…¥
  â†“                     â†“
å›ºå®šé€»è¾‘             æ™ºèƒ½æ¨ç† (LLM)
  â†“                     â†“
æ•°æ®åº“æŸ¥è¯¢           å·¥å…·è°ƒç”¨ (Tools)
  â†“                     â†“
è¿”å›ç»“æœ             å¤–éƒ¨æ•°æ® (RAG)
                        â†“
                     ç»¼åˆå›ç­”
```

**ä¼ ç»Ÿå¼€å‘ vs LangChain å¼€å‘**ï¼š

| ä¼ ç»Ÿæ–¹å¼ | LangChain æ–¹å¼ |
|---------|---------------|
| ç¡¬ç¼–ç ä¸šåŠ¡é€»è¾‘ | LLM åŠ¨æ€æ¨ç† |
| å›ºå®šçš„ if-else | Agent è‡ªä¸»å†³ç­– |
| å•ä¸€æ•°æ®æº | å¤šæ•°æ®æºé›†æˆï¼ˆRAGï¼‰|
| æ— ä¸Šä¸‹æ–‡ | å®Œæ•´çš„è®°å¿†ç³»ç»Ÿ |
| éš¾ä»¥ç»´æŠ¤ | æ¨¡å—åŒ–å¯æ‰©å±• |

### 1.2 ä¸ºä»€ä¹ˆéœ€è¦ LangChainï¼Ÿ

#### é—®é¢˜ 1ï¼šLLM å•ç‹¬ä½¿ç”¨çš„å±€é™æ€§

```python
# âŒ ç›´æ¥ä½¿ç”¨ OpenAI API çš„é—®é¢˜
import openai

# é—®é¢˜ 1: æ²¡æœ‰è®°å¿†ï¼Œæ¯æ¬¡éƒ½æ˜¯æ–°å¯¹è¯
response1 = openai.chat.completions.create(
    model="gpt-4",
    messages=[{"role": "user", "content": "æˆ‘å« Alice"}]
)

# ä¸‹ä¸€æ¬¡è°ƒç”¨ï¼Œæ¨¡å‹ä¸è®°å¾—ç”¨æˆ·æ˜¯ Alice
response2 = openai.chat.completions.create(
    model="gpt-4",
    messages=[{"role": "user", "content": "æˆ‘å«ä»€ä¹ˆåå­—ï¼Ÿ"}]
)
# å›ç­”ï¼šæˆ‘ä¸çŸ¥é“ä½ çš„åå­—

# é—®é¢˜ 2: æ— æ³•è®¿é—®å¤–éƒ¨æ•°æ®
response3 = openai.chat.completions.create(
    model="gpt-4",
    messages=[{"role": "user", "content": "2024å¹´11æœˆçš„æ–°é—»æœ‰å“ªäº›ï¼Ÿ"}]
)
# å›ç­”ï¼šæˆ‘çš„çŸ¥è¯†æˆªæ­¢åˆ° 2023 å¹´...

# é—®é¢˜ 3: æ— æ³•è°ƒç”¨å·¥å…·
response4 = openai.chat.completions.create(
    model="gpt-4",
    messages=[{"role": "user", "content": "å¸®æˆ‘å‘é€ä¸€å°é‚®ä»¶"}]
)
# å›ç­”ï¼šæˆ‘æ˜¯ AIï¼Œæ— æ³•å‘é€é‚®ä»¶ï¼ˆåªèƒ½æä¾›å»ºè®®ï¼‰
```

#### LangChain çš„è§£å†³æ–¹æ¡ˆ

```python
# âœ… ä½¿ç”¨ LangChain è§£å†³è¿™äº›é—®é¢˜
from langchain.agents import create_agent
from langchain.tools import tool
from langgraph.checkpoint.memory import InMemorySaver

# è§£å†³é—®é¢˜ 1: æ·»åŠ è®°å¿†ç³»ç»Ÿ
checkpointer = InMemorySaver()

# è§£å†³é—®é¢˜ 2: å®šä¹‰æ£€ç´¢å·¥å…·ï¼ˆè®¿é—®å¤–éƒ¨æ•°æ®ï¼‰
@tool
def search_news(query: str) -> str:
    """æœç´¢æœ€æ–°æ–°é—»"""
    # è°ƒç”¨æ–°é—» API
    return "2024å¹´11æœˆæœ€æ–°æ–°é—»ï¼š..."

# è§£å†³é—®é¢˜ 3: å®šä¹‰åŠŸèƒ½å·¥å…·
@tool
def send_email(to: str, subject: str, body: str) -> str:
    """å‘é€é‚®ä»¶"""
    # è°ƒç”¨é‚®ä»¶æœåŠ¡ API
    return f"é‚®ä»¶å·²å‘é€åˆ° {to}"

# åˆ›å»ºå…·å¤‡æ‰€æœ‰èƒ½åŠ›çš„ Agent
agent = create_agent(
    model="gpt-4",
    tools=[search_news, send_email],
    checkpointer=checkpointer  # è®°å¿†ç³»ç»Ÿ
)

# ç¬¬ä¸€æ¬¡å¯¹è¯
agent.invoke(
    {"messages": [{"role": "user", "content": "æˆ‘å« Alice"}]},
    {"configurable": {"thread_id": "conversation_1"}}
)

# ç¬¬äºŒæ¬¡å¯¹è¯ - Agent ä¼šè®°ä½ Alice
agent.invoke(
    {"messages": [{"role": "user", "content": "æˆ‘å«ä»€ä¹ˆåå­—ï¼Ÿ"}]},
    {"configurable": {"thread_id": "conversation_1"}}
)
# å›ç­”ï¼šä½ çš„åå­—æ˜¯ Alice

# Agent å¯ä»¥æœç´¢æ–°é—»
agent.invoke(
    {"messages": [{"role": "user", "content": "2024å¹´11æœˆçš„æ–°é—»"}]},
    {"configurable": {"thread_id": "conversation_1"}}
)
# Agent ä¼šè°ƒç”¨ search_news å·¥å…·

# Agent å¯ä»¥å‘é€é‚®ä»¶
agent.invoke(
    {"messages": [{"role": "user", "content": "å‘é‚®ä»¶ç»™ bob@example.com"}]},
    {"configurable": {"thread_id": "conversation_1"}}
)
# Agent ä¼šè°ƒç”¨ send_email å·¥å…·
```

### 1.3 LangChain çš„æ ¸å¿ƒä»·å€¼

#### ä»·å€¼ 1ï¼šç»Ÿä¸€çš„æŠ½è±¡å±‚

LangChain æä¾›ç»Ÿä¸€çš„æ¥å£æ¥ä½¿ç”¨ä¸åŒçš„ LLM æä¾›å•†ï¼š

```python
from langchain.chat_models import init_chat_model

# ä½¿ç”¨ OpenAI
model = init_chat_model("gpt-4")
response = model.invoke("ä½ å¥½")

# åˆ‡æ¢åˆ° Anthropic Claudeï¼ˆä»£ç å®Œå…¨ç›¸åŒï¼‰
model = init_chat_model("claude-sonnet-4-5-20250929")
response = model.invoke("ä½ å¥½")

# åˆ‡æ¢åˆ° Google Geminiï¼ˆä»£ç å®Œå…¨ç›¸åŒï¼‰
model = init_chat_model("google_genai:gemini-2.5-flash-lite")
response = model.invoke("ä½ å¥½")

# ğŸ’¡ å…³é”®ä¼˜åŠ¿ï¼šæ— éœ€ä¿®æ”¹ä»£ç å³å¯åˆ‡æ¢æ¨¡å‹
```

#### ä»·å€¼ 2ï¼šç»„ä»¶åŒ–å’Œå¯å¤ç”¨

```python
# å®šä¹‰å¯å¤ç”¨çš„ç»„ä»¶
from langchain.tools import tool

# ç»„ä»¶ 1: å¤©æ°”å·¥å…·
@tool
def get_weather(city: str) -> str:
    """è·å–åŸå¸‚å¤©æ°”"""
    return f"{city}çš„å¤©æ°”ï¼šæ™´å¤© 22Â°C"

# ç»„ä»¶ 2: ç¿»è¯‘å·¥å…·
@tool
def translate(text: str, target_lang: str) -> str:
    """ç¿»è¯‘æ–‡æœ¬"""
    return f"ç¿»è¯‘åçš„æ–‡æœ¬ï¼š{text} -> {target_lang}"

# å¯ä»¥åœ¨ä¸åŒçš„ Agent ä¸­å¤ç”¨è¿™äº›å·¥å…·
agent1 = create_agent(
    model="gpt-4",
    tools=[get_weather]  # åªæœ‰å¤©æ°”åŠŸèƒ½
)

agent2 = create_agent(
    model="gpt-4",
    tools=[get_weather, translate]  # å¤©æ°” + ç¿»è¯‘
)

# ğŸ’¡ å…³é”®ä¼˜åŠ¿ï¼šå·¥å…·å¯ä»¥åœ¨å¤šä¸ª Agent ä¹‹é—´å…±äº«å’Œå¤ç”¨
```

#### ä»·å€¼ 3ï¼šå¼ºå¤§çš„ç¼–æ’èƒ½åŠ›

```python
from langgraph.graph import StateGraph, START, END
from typing_extensions import TypedDict

# å®šä¹‰å·¥ä½œæµçŠ¶æ€
class State(TypedDict):
    user_input: str
    weather: str
    translation: str
    final_output: str

# å®šä¹‰å·¥ä½œæµèŠ‚ç‚¹
def get_weather_node(state: State):
    weather = f"{state['user_input']} çš„å¤©æ°”ï¼šæ™´å¤©"
    return {"weather": weather}

def translate_node(state: State):
    translation = f"ç¿»è¯‘ï¼š{state['weather']}"
    return {"translation": translation}

def format_output_node(state: State):
    output = f"æœ€ç»ˆç»“æœï¼š{state['translation']}"
    return {"final_output": output}

# æ„å»ºå·¥ä½œæµ
workflow = StateGraph(State)
workflow.add_node("get_weather", get_weather_node)
workflow.add_node("translate", translate_node)
workflow.add_node("format", format_output_node)

# å®šä¹‰æ‰§è¡Œé¡ºåº
workflow.add_edge(START, "get_weather")
workflow.add_edge("get_weather", "translate")
workflow.add_edge("translate", "format")
workflow.add_edge("format", END)

# ç¼–è¯‘å¹¶æ‰§è¡Œ
app = workflow.compile()
result = app.invoke({"user_input": "åŒ—äº¬"})
print(result["final_output"])

# ğŸ’¡ å…³é”®ä¼˜åŠ¿ï¼šå¯ä»¥ç¼–æ’å¤æ‚çš„å¤šæ­¥éª¤å·¥ä½œæµ
```

### 1.4 åº”ç”¨åœºæ™¯è¯¦è§£

#### åœºæ™¯ 1ï¼šæ™ºèƒ½å®¢æœç³»ç»Ÿ

```python
from langchain.agents import create_agent
from langchain.tools import tool
from langgraph.checkpoint.memory import InMemorySaver

# å®¢æœå·¥å…·
@tool
def query_order(order_id: str) -> str:
    """æŸ¥è¯¢è®¢å•çŠ¶æ€"""
    return f"è®¢å• {order_id}ï¼šå·²å‘è´§ï¼Œé¢„è®¡æ˜å¤©é€è¾¾"

@tool
def cancel_order(order_id: str) -> str:
    """å–æ¶ˆè®¢å•"""
    return f"è®¢å• {order_id} å·²å–æ¶ˆ"

@tool
def search_faq(question: str) -> str:
    """æœç´¢å¸¸è§é—®é¢˜"""
    return "æ ¹æ®æ‚¨çš„é—®é¢˜ï¼Œå»ºè®®æŸ¥çœ‹é€€è´§æ”¿ç­–..."

# åˆ›å»ºå®¢æœ Agent
customer_service_agent = create_agent(
    model="gpt-4",
    tools=[query_order, cancel_order, search_faq],
    system_prompt="""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å®¢æœåŠ©æ‰‹ã€‚
    - å‹å¥½ã€è€å¿ƒã€ä¸“ä¸š
    - å…ˆç†è§£ç”¨æˆ·é—®é¢˜ï¼Œå†å†³å®šä½¿ç”¨å“ªä¸ªå·¥å…·
    - æä¾›å‡†ç¡®çš„è®¢å•ä¿¡æ¯
    """,
    checkpointer=InMemorySaver()
)

# æ¨¡æ‹Ÿå®¢æœå¯¹è¯
response = customer_service_agent.invoke(
    {"messages": [{"role": "user", "content": "æˆ‘çš„è®¢å• 12345 ä»€ä¹ˆæ—¶å€™åˆ°ï¼Ÿ"}]},
    {"configurable": {"thread_id": "customer_001"}}
)

# Agent ä¼šï¼š
# 1. ç†è§£ç”¨æˆ·æƒ³æŸ¥è¯¢è®¢å•çŠ¶æ€
# 2. è°ƒç”¨ query_order("12345")
# 3. è¿”å›å‹å¥½çš„ç­”æ¡ˆ
```

#### åœºæ™¯ 2ï¼šä»£ç åŠ©æ‰‹

```python
@tool
def execute_code(code: str) -> str:
    """æ‰§è¡Œ Python ä»£ç """
    try:
        exec_result = exec(code)
        return f"æ‰§è¡ŒæˆåŠŸï¼š{exec_result}"
    except Exception as e:
        return f"æ‰§è¡Œé”™è¯¯ï¼š{e}"

@tool
def search_documentation(library: str, topic: str) -> str:
    """æœç´¢åº“æ–‡æ¡£"""
    return f"{library} çš„ {topic} æ–‡æ¡£ï¼š..."

code_assistant = create_agent(
    model="gpt-4",
    tools=[execute_code, search_documentation],
    system_prompt="ä½ æ˜¯ä¸€ä¸ª Python ç¼–ç¨‹åŠ©æ‰‹ï¼Œå¸®åŠ©ç”¨æˆ·ç¼–å†™å’Œè°ƒè¯•ä»£ç "
)

# ç”¨æˆ·ï¼šå¸®æˆ‘å†™ä¸€ä¸ªå¿«é€Ÿæ’åº
response = code_assistant.invoke({
    "messages": [{"role": "user", "content": "å¸®æˆ‘å†™ä¸€ä¸ªå¿«é€Ÿæ’åºç®—æ³•"}]
})
# Agent ä¼šç”Ÿæˆä»£ç å¹¶å¯ä»¥æ‰§è¡ŒéªŒè¯
```

#### åœºæ™¯ 3ï¼šæ–‡æ¡£åˆ†æï¼ˆRAGï¼‰

```python
from langchain_core.vectorstores import InMemoryVectorStore
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import PyPDFLoader

# åŠ è½½æ–‡æ¡£
loader = PyPDFLoader("company_handbook.pdf")
documents = loader.load()

# åˆ†å‰²æ–‡æ¡£
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200
)
splits = text_splitter.split_documents(documents)

# åˆ›å»ºå‘é‡å­˜å‚¨
vector_store = InMemoryVectorStore.from_documents(
    documents=splits,
    embedding=embeddings
)

# å®šä¹‰æ£€ç´¢å·¥å…·
@tool
def search_handbook(query: str) -> str:
    """æœç´¢å…¬å¸æ‰‹å†Œ"""
    docs = vector_store.similarity_search(query, k=3)
    return "\n\n".join([doc.page_content for doc in docs])

# åˆ›å»ºæ–‡æ¡£é—®ç­” Agent
handbook_qa = create_agent(
    model="gpt-4",
    tools=[search_handbook],
    system_prompt="ä½ æ˜¯å…¬å¸æ‰‹å†ŒåŠ©æ‰‹ï¼ŒåŸºäºæ£€ç´¢åˆ°çš„å†…å®¹å‡†ç¡®å›ç­”é—®é¢˜"
)

# ç”¨æˆ·æé—®
response = handbook_qa.invoke({
    "messages": [{"role": "user", "content": "å…¬å¸çš„ä¼‘å‡æ”¿ç­–æ˜¯ä»€ä¹ˆï¼Ÿ"}]
})
# Agent ä¼šï¼š
# 1. è°ƒç”¨ search_handbook æ£€ç´¢ç›¸å…³å†…å®¹
# 2. åŸºäºæ£€ç´¢ç»“æœç”Ÿæˆå›ç­”
```

#### åœºæ™¯ 4ï¼šæ•°æ®åˆ†æåŠ©æ‰‹

```python
import pandas as pd

@tool
def load_dataset(file_path: str) -> str:
    """åŠ è½½æ•°æ®é›†"""
    df = pd.read_csv(file_path)
    return f"æ•°æ®é›†åŠ è½½æˆåŠŸï¼Œå…± {len(df)} è¡Œï¼Œ{len(df.columns)} åˆ—"

@tool
def analyze_data(analysis_type: str, column: str) -> str:
    """åˆ†ææ•°æ®"""
    # è¿™é‡Œç®€åŒ–å¤„ç†
    return f"å¯¹ {column} åˆ—è¿›è¡Œ {analysis_type} åˆ†æçš„ç»“æœï¼š..."

@tool
def generate_visualization(chart_type: str, x_column: str, y_column: str) -> str:
    """ç”Ÿæˆå¯è§†åŒ–"""
    return f"å·²ç”Ÿæˆ {chart_type} å›¾è¡¨ï¼š{x_column} vs {y_column}"

data_analyst = create_agent(
    model="gpt-4",
    tools=[load_dataset, analyze_data, generate_visualization],
    system_prompt="ä½ æ˜¯æ•°æ®åˆ†æåŠ©æ‰‹ï¼Œå¸®åŠ©ç”¨æˆ·åˆ†ææ•°æ®å’Œç”Ÿæˆå¯è§†åŒ–"
)

# ç”¨æˆ·ï¼šåˆ†æé”€å”®æ•°æ®
response = data_analyst.invoke({
    "messages": [{"role": "user", "content": "åŠ è½½ sales.csv å¹¶åˆ†æé”€å”®è¶‹åŠ¿"}]
})
# Agent ä¼šæŒ‰æ­¥éª¤ï¼š
# 1. load_dataset("sales.csv")
# 2. analyze_data("è¶‹åŠ¿åˆ†æ", "é”€å”®é¢")
# 3. generate_visualization("æŠ˜çº¿å›¾", "æ—¥æœŸ", "é”€å”®é¢")
```

### 1.5 LangChain ç”Ÿæ€ç³»ç»Ÿ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         LangChain ç”Ÿæ€ç³»ç»Ÿ               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚ LangChain  â”‚  â”‚ LangGraph  â”‚        â”‚
â”‚  â”‚ æ ¸å¿ƒç»„ä»¶    â”‚  â”‚ å·¥ä½œæµç¼–æ’  â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚ LangSmith  â”‚  â”‚ LangServe  â”‚        â”‚
â”‚  â”‚ ç›‘æ§è¿½è¸ª    â”‚  â”‚ éƒ¨ç½²æœåŠ¡    â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚ Community  â”‚  â”‚ Integrationsâ”‚        â”‚
â”‚  â”‚ ç¤¾åŒºè´¡çŒ®    â”‚  â”‚ ç¬¬ä¸‰æ–¹é›†æˆ   â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**æ ¸å¿ƒåº“è¯´æ˜**ï¼š

| åº“å | ç”¨é€” | ä½•æ—¶ä½¿ç”¨ |
|-----|------|---------|
| **langchain** | æ ¸å¿ƒç»„ä»¶ï¼ˆModels, Tools, Promptsï¼‰ | æ„å»ºåŸºç¡€ LLM åº”ç”¨ |
| **langgraph** | çŠ¶æ€å·¥ä½œæµç¼–æ’ | å¤æ‚å¤šæ­¥éª¤ä»»åŠ¡ã€æ¡ä»¶è·¯ç”± |
| **langsmith** | ç›‘æ§ã€è¿½è¸ªã€è¯„ä¼° | ç”Ÿäº§ç¯å¢ƒç›‘æ§å’Œè°ƒè¯• |
| **langserve** | éƒ¨ç½² REST API | å°† Chain/Agent éƒ¨ç½²ä¸ºæœåŠ¡ |
| **langchain-community** | ç¤¾åŒºé›†æˆ | ä½¿ç”¨ç¬¬ä¸‰æ–¹å·¥å…·å’ŒæœåŠ¡ |

---

## ç¬¬äºŒéƒ¨åˆ†ï¼šå®‰è£…ä¸ç¯å¢ƒé…ç½®

### 2.1 åŸºç¡€å®‰è£…

#### æœ€å°åŒ–å®‰è£…

```bash
# åªå®‰è£…æ ¸å¿ƒåº“
pip install langchain langchain-core

# éªŒè¯å®‰è£…
python -c "import langchain; print(langchain.__version__)"
```

#### å®Œæ•´å®‰è£…ï¼ˆæ¨èï¼‰

```bash
# æ ¸å¿ƒåº“
pip install langchain langchain-core

# æ–‡æœ¬åˆ†å‰²å™¨
pip install langchain-text-splitters

# ç¤¾åŒºé›†æˆ
pip install langchain-community

# å·¥ä½œæµç¼–æ’
pip install langgraph

# ç›‘æ§è¿½è¸ª
pip install langsmith

# å¸¸ç”¨å·¥å…·
pip install beautifulsoup4  # ç½‘é¡µåŠ è½½
pip install faiss-cpu       # å‘é‡å­˜å‚¨
```

#### ä½¿ç”¨ requirements.txt

åˆ›å»º `requirements.txt` æ–‡ä»¶ï¼š

```txt
# requirements.txt
langchain>=0.1.0
langchain-core>=0.1.0
langchain-community>=0.0.20
langchain-text-splitters>=0.0.1
langgraph>=0.0.40
langsmith>=0.0.70

# æ¨¡å‹æä¾›å•†ï¼ˆæ ¹æ®éœ€è¦é€‰æ‹©ï¼‰
langchain-openai>=0.0.5
langchain-anthropic>=0.0.5
langchain-google-genai>=0.0.5

# å·¥å…·åº“
beautifulsoup4>=4.12.0
faiss-cpu>=1.7.4
```

å®‰è£…ï¼š
```bash
pip install -r requirements.txt
```

### 2.2 å„æ¨¡å‹æä¾›å•†é›†æˆ

#### OpenAI

```bash
# å®‰è£…
pip install langchain-openai
```

```python
import os
from langchain_openai import ChatOpenAI

# è®¾ç½® API Key
os.environ["OPENAI_API_KEY"] = "sk-..."

# åˆå§‹åŒ–æ¨¡å‹
model = ChatOpenAI(
    model="gpt-4",
    temperature=0.7
)

# æµ‹è¯•è°ƒç”¨
response = model.invoke("ä½ å¥½ï¼Œä¸–ç•Œï¼")
print(response.content)
```

#### Anthropic Claude

```bash
# å®‰è£…
pip install langchain-anthropic
```

```python
import os
from langchain_anthropic import ChatAnthropic

# è®¾ç½® API Key
os.environ["ANTHROPIC_API_KEY"] = "sk-ant-..."

# åˆå§‹åŒ–æ¨¡å‹
model = ChatAnthropic(
    model="claude-sonnet-4-5-20250929",
    temperature=0.7,
    max_tokens=1024
)

# æµ‹è¯•è°ƒç”¨
response = model.invoke("ä½ å¥½ï¼ŒClaudeï¼")
print(response.content)
```

#### Google Gemini

```bash
# å®‰è£…
pip install langchain-google-genai
```

```python
import os
from langchain_google_genai import ChatGoogleGenerativeAI

# è®¾ç½® API Key
os.environ["GOOGLE_API_KEY"] = "..."

# åˆå§‹åŒ–æ¨¡å‹
model = ChatGoogleGenerativeAI(
    model="gemini-2.5-flash-lite",
    temperature=0.7
)

# æµ‹è¯•è°ƒç”¨
response = model.invoke("ä½ å¥½ï¼ŒGeminiï¼")
print(response.content)
```

#### Azure OpenAI

```bash
# å®‰è£…
pip install langchain-openai
```

```python
import os
from langchain_openai import AzureChatOpenAI

# è®¾ç½®ç¯å¢ƒå˜é‡
os.environ["AZURE_OPENAI_API_KEY"] = "..."
os.environ["AZURE_OPENAI_ENDPOINT"] = "https://your-resource.openai.azure.com/"
os.environ["OPENAI_API_VERSION"] = "2024-02-15-preview"

# åˆå§‹åŒ–æ¨¡å‹
model = AzureChatOpenAI(
    model="gpt-4",
    azure_deployment="your-deployment-name",
    temperature=0.7
)

# æµ‹è¯•è°ƒç”¨
response = model.invoke("ä½ å¥½ï¼ŒAzure OpenAIï¼")
print(response.content)
```

### 2.3 ç¯å¢ƒå˜é‡é…ç½®æœ€ä½³å®è·µ

#### ä½¿ç”¨ .env æ–‡ä»¶ï¼ˆæ¨èï¼‰

åˆ›å»º `.env` æ–‡ä»¶ï¼š

```bash
# .env æ–‡ä»¶

# OpenAI
OPENAI_API_KEY=sk-...

# Anthropic
ANTHROPIC_API_KEY=sk-ant-...

# Google
GOOGLE_API_KEY=...

# LangSmithï¼ˆå¯é€‰ï¼‰
LANGSMITH_API_KEY=...
LANGSMITH_TRACING=true
LANGSMITH_PROJECT=my-project
```

ä½¿ç”¨ python-dotenv åŠ è½½ï¼š

```bash
pip install python-dotenv
```

```python
from dotenv import load_dotenv
import os

# åŠ è½½ .env æ–‡ä»¶
load_dotenv()

# ç°åœ¨å¯ä»¥ä½¿ç”¨ç¯å¢ƒå˜é‡
api_key = os.getenv("OPENAI_API_KEY")

from langchain_openai import ChatOpenAI
model = ChatOpenAI()  # è‡ªåŠ¨ä»ç¯å¢ƒå˜é‡è¯»å– API Key
```

#### ä½¿ç”¨ getpassï¼ˆäº¤äº’å¼è¾“å…¥ï¼‰

```python
import getpass
import os

# å¦‚æœç¯å¢ƒå˜é‡ä¸å­˜åœ¨ï¼Œæç¤ºç”¨æˆ·è¾“å…¥
if not os.environ.get("OPENAI_API_KEY"):
    os.environ["OPENAI_API_KEY"] = getpass.getpass("è¯·è¾“å…¥ OpenAI API Key: ")

from langchain_openai import ChatOpenAI
model = ChatOpenAI()
```

### 2.4 éªŒè¯å®‰è£…

åˆ›å»ºæµ‹è¯•è„šæœ¬ `test_installation.py`ï¼š

```python
"""
LangChain å®‰è£…éªŒè¯è„šæœ¬
"""

def test_basic_import():
    """æµ‹è¯•åŸºç¡€å¯¼å…¥"""
    try:
        import langchain
        import langchain_core
        import langgraph
        print("âœ… åŸºç¡€åº“å¯¼å…¥æˆåŠŸ")
        print(f"   LangChain ç‰ˆæœ¬: {langchain.__version__}")
        return True
    except ImportError as e:
        print(f"âŒ åŸºç¡€åº“å¯¼å…¥å¤±è´¥: {e}")
        return False

def test_openai():
    """æµ‹è¯• OpenAI"""
    try:
        from langchain_openai import ChatOpenAI
        import os
        
        if not os.environ.get("OPENAI_API_KEY"):
            print("âš ï¸  OPENAI_API_KEY æœªè®¾ç½®ï¼Œè·³è¿‡æµ‹è¯•")
            return True
        
        model = ChatOpenAI(model="gpt-3.5-turbo")
        response = model.invoke("æµ‹è¯•")
        print("âœ… OpenAI è¿æ¥æˆåŠŸ")
        return True
    except Exception as e:
        print(f"âŒ OpenAI è¿æ¥å¤±è´¥: {e}")
        return False

def test_anthropic():
    """æµ‹è¯• Anthropic"""
    try:
        from langchain_anthropic import ChatAnthropic
        import os
        
        if not os.environ.get("ANTHROPIC_API_KEY"):
            print("âš ï¸  ANTHROPIC_API_KEY æœªè®¾ç½®ï¼Œè·³è¿‡æµ‹è¯•")
            return True
        
        model = ChatAnthropic(model="claude-3-haiku-20240307")
        response = model.invoke("æµ‹è¯•")
        print("âœ… Anthropic è¿æ¥æˆåŠŸ")
        return True
    except Exception as e:
        print(f"âŒ Anthropic è¿æ¥å¤±è´¥: {e}")
        return False

def test_agent_creation():
    """æµ‹è¯• Agent åˆ›å»º"""
    try:
        from langchain.agents import create_agent
        from langchain.tools import tool
        
        @tool
        def dummy_tool(input: str) -> str:
            """æµ‹è¯•å·¥å…·"""
            return "æµ‹è¯•ç»“æœ"
        
        agent = create_agent(
            model="gpt-3.5-turbo",
            tools=[dummy_tool]
        )
        print("âœ… Agent åˆ›å»ºæˆåŠŸ")
        return True
    except Exception as e:
        print(f"âŒ Agent åˆ›å»ºå¤±è´¥: {e}")
        return False

if __name__ == "__main__":
    print("=" * 50)
    print("å¼€å§‹éªŒè¯ LangChain å®‰è£…...")
    print("=" * 50)
    
    tests = [
        test_basic_import,
        test_openai,
        test_anthropic,
        test_agent_creation
    ]
    
    results = [test() for test in tests]
    
    print("=" * 50)
    print(f"æµ‹è¯•å®Œæˆï¼š{sum(results)}/{len(results)} é€šè¿‡")
    print("=" * 50)
```

è¿è¡ŒéªŒè¯ï¼š
```bash
python test_installation.py
```

---

## ç¬¬ä¸‰éƒ¨åˆ†ï¼šæ ¸å¿ƒæ¶æ„æ·±å…¥ç†è§£

### 3.1 Runnable æ¥å£è¯¦è§£

**Runnable** æ˜¯ LangChain ä¸­æ‰€æœ‰ç»„ä»¶çš„åŸºç¡€æ¥å£ã€‚ç†è§£ Runnable æ˜¯æŒæ¡ LangChain çš„å…³é”®ã€‚

#### ä»€ä¹ˆæ˜¯ Runnableï¼Ÿ

```python
from langchain_core.runnables import Runnable

# æ‰€æœ‰è¿™äº›éƒ½æ˜¯ Runnableï¼š
# - Model (ChatOpenAI, ChatAnthropic, ...)
# - PromptTemplate
# - OutputParser
# - Tool
# - Chain
# - Agent
```

#### Runnable çš„æ ¸å¿ƒæ–¹æ³•

```python
from langchain_openai import ChatOpenAI

model = ChatOpenAI(model="gpt-3.5-turbo")

# 1. invoke() - å•æ¬¡åŒæ­¥è°ƒç”¨
response = model.invoke("ä½ å¥½")
print(response.content)

# 2. batch() - æ‰¹é‡å¤„ç†
responses = model.batch([
    "ç¬¬ä¸€ä¸ªé—®é¢˜",
    "ç¬¬äºŒä¸ªé—®é¢˜",
    "ç¬¬ä¸‰ä¸ªé—®é¢˜"
])
for resp in responses:
    print(resp.content)

# 3. stream() - æµå¼è¾“å‡º
for chunk in model.stream("ç»™æˆ‘è®²ä¸ªé•¿æ•…äº‹"):
    print(chunk.content, end="", flush=True)

# 4. ainvoke() - å¼‚æ­¥è°ƒç”¨
import asyncio

async def async_call():
    response = await model.ainvoke("ä½ å¥½")
    print(response.content)

asyncio.run(async_call())

# 5. abatch() - å¼‚æ­¥æ‰¹é‡
async def async_batch():
    responses = await model.abatch(["é—®é¢˜1", "é—®é¢˜2"])
    for resp in responses:
        print(resp.content)

asyncio.run(async_batch())

# 6. astream() - å¼‚æ­¥æµå¼
async def async_stream():
    async for chunk in model.astream("è®²ä¸ªæ•…äº‹"):
        print(chunk.content, end="", flush=True)

asyncio.run(async_stream())
```

#### Runnable æ–¹æ³•å¯¹æ¯”

| æ–¹æ³• | åŒæ­¥/å¼‚æ­¥ | è¿”å›ç±»å‹ | ä½¿ç”¨åœºæ™¯ |
|------|----------|---------|---------|
| `invoke()` | åŒæ­¥ | å•ä¸ªç»“æœ | å•ä¸ªè¯·æ±‚ï¼Œç­‰å¾…å®Œæˆ |
| `batch()` | åŒæ­¥ | ç»“æœåˆ—è¡¨ | å¤šä¸ªè¯·æ±‚ï¼Œæ‰¹é‡å¤„ç† |
| `stream()` | åŒæ­¥ | ç”Ÿæˆå™¨ | é•¿æ–‡æœ¬ï¼Œå®æ—¶æ˜¾ç¤º |
| `ainvoke()` | å¼‚æ­¥ | å•ä¸ªç»“æœ | å¼‚æ­¥ç¯å¢ƒï¼Œå•ä¸ªè¯·æ±‚ |
| `abatch()` | å¼‚æ­¥ | ç»“æœåˆ—è¡¨ | å¼‚æ­¥ç¯å¢ƒï¼Œæ‰¹é‡è¯·æ±‚ |
| `astream()` | å¼‚æ­¥ | å¼‚æ­¥ç”Ÿæˆå™¨ | å¼‚æ­¥ç¯å¢ƒï¼Œæµå¼è¾“å‡º |

#### å®æˆ˜ç¤ºä¾‹ï¼šæ‰¹é‡ç¿»è¯‘

```python
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate

# åˆ›å»ºç¿»è¯‘æç¤ºæ¨¡æ¿
prompt = ChatPromptTemplate.from_messages([
    ("system", "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç¿»è¯‘åŠ©æ‰‹"),
    ("user", "å°†ä»¥ä¸‹æ–‡æœ¬ç¿»è¯‘æˆè‹±æ–‡ï¼š{text}")
])

model = ChatOpenAI(model="gpt-3.5-turbo")

# åˆ›å»ºç¿»è¯‘é“¾
translation_chain = prompt | model

# æ‰¹é‡ç¿»è¯‘
texts_to_translate = [
    {"text": "ä½ å¥½ï¼Œä¸–ç•Œ"},
    {"text": "ä»Šå¤©å¤©æ°”çœŸå¥½"},
    {"text": "æˆ‘å–œæ¬¢ç¼–ç¨‹"}
]

# ä½¿ç”¨ batch æ–¹æ³•
translations = translation_chain.batch(texts_to_translate)

for original, translation in zip(texts_to_translate, translations):
    print(f"åŸæ–‡: {original['text']}")
    print(f"è¯‘æ–‡: {translation.content}")
    print("-" * 50)
```

### 3.2 LCEL è¡¨è¾¾å¼è¯­è¨€

**LCEL (LangChain Expression Language)** æ˜¯ä¸€ç§å£°æ˜å¼çš„æ–¹å¼æ¥ç»„åˆ LangChain ç»„ä»¶ã€‚

#### åŸºç¡€è¯­æ³•

```python
# LCEL ä½¿ç”¨ | æ“ä½œç¬¦è¿æ¥ç»„ä»¶
chain = component1 | component2 | component3

# ç­‰ä»·äºå‡½æ•°ç»„åˆ
result = component3(component2(component1(input)))
```

#### ç¤ºä¾‹ 1ï¼šç®€å•çš„æç¤º â†’ æ¨¡å‹ â†’ è§£æé“¾

```python
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

# ç»„ä»¶ 1: æç¤ºæ¨¡æ¿
prompt = ChatPromptTemplate.from_messages([
    ("system", "ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„åŠ©æ‰‹"),
    ("user", "{question}")
])

# ç»„ä»¶ 2: æ¨¡å‹
model = ChatOpenAI(model="gpt-3.5-turbo")

# ç»„ä»¶ 3: è¾“å‡ºè§£æå™¨
output_parser = StrOutputParser()

# ä½¿ç”¨ LCEL ç»„åˆ
chain = prompt | model | output_parser

# è°ƒç”¨é“¾
result = chain.invoke({"question": "ä»€ä¹ˆæ˜¯é‡å­è®¡ç®—ï¼Ÿ"})
print(result)  # ç›´æ¥å¾—åˆ°å­—ç¬¦ä¸²ç»“æœ
```

#### ç¤ºä¾‹ 2ï¼šå¤šæ­¥éª¤å¤„ç†é“¾

```python
from langchain_core.runnables import RunnableLambda

# è‡ªå®šä¹‰å¤„ç†å‡½æ•°
def to_uppercase(text: str) -> str:
    """è½¬å¤§å†™"""
    return text.upper()

def add_prefix(text: str) -> str:
    """æ·»åŠ å‰ç¼€"""
    return f"[å¤„ç†ç»“æœ] {text}"

# åˆ›å»º Runnable
uppercase_runnable = RunnableLambda(to_uppercase)
prefix_runnable = RunnableLambda(add_prefix)

# ç»„åˆé“¾
processing_chain = prompt | model | output_parser | uppercase_runnable | prefix_runnable

result = processing_chain.invoke({"question": "hello"})
print(result)
# è¾“å‡º: [å¤„ç†ç»“æœ] HELLO, I'M AN AI ASSISTANT...
```

#### ç¤ºä¾‹ 3ï¼šæ¡ä»¶è·¯ç”±

```python
from langchain_core.runnables import RunnableBranch

# å®šä¹‰ä¸åŒçš„å¤„ç†è·¯å¾„
def is_question(input_dict):
    """åˆ¤æ–­æ˜¯å¦æ˜¯é—®é¢˜"""
    return "?" in input_dict.get("text", "")

# è·¯ç”± 1: é—®é¢˜å¤„ç†é“¾
question_chain = (
    ChatPromptTemplate.from_template("å›ç­”é—®é¢˜ï¼š{text}")
    | model
    | StrOutputParser()
)

# è·¯ç”± 2: é™ˆè¿°å¤„ç†é“¾
statement_chain = (
    ChatPromptTemplate.from_template("æ€»ç»“é™ˆè¿°ï¼š{text}")
    | model
    | StrOutputParser()
)

# åˆ›å»ºåˆ†æ”¯è·¯ç”±
branch = RunnableBranch(
    (is_question, question_chain),  # å¦‚æœæ˜¯é—®é¢˜ï¼Œä½¿ç”¨é—®é¢˜é“¾
    statement_chain  # å¦åˆ™ä½¿ç”¨é™ˆè¿°é“¾
)

# æµ‹è¯•
print(branch.invoke({"text": "ä»€ä¹ˆæ˜¯AIï¼Ÿ"}))  # ä½¿ç”¨é—®é¢˜é“¾
print(branch.invoke({"text": "ä»Šå¤©å¤©æ°”å¾ˆå¥½ã€‚"}))  # ä½¿ç”¨é™ˆè¿°é“¾
```

### 3.3 é“¾å¼è°ƒç”¨æœºåˆ¶

#### ç†è§£æ•°æ®æµ

```python
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

prompt = ChatPromptTemplate.from_template("è®²ä¸€ä¸ªå…³äº{topic}çš„ç¬‘è¯")
model = ChatOpenAI()
parser = StrOutputParser()

chain = prompt | model | parser

# æ•°æ®æµå‘ï¼š
# 1. {"topic": "ç¨‹åºå‘˜"} 
#     â†“
# 2. prompt.invoke() â†’ "è®²ä¸€ä¸ªå…³äºç¨‹åºå‘˜çš„ç¬‘è¯"
#     â†“
# 3. model.invoke() â†’ AIMessage(content="...")
#     â†“
# 4. parser.invoke() â†’ "ç¬‘è¯å†…å®¹å­—ç¬¦ä¸²"
```

#### è°ƒè¯•é“¾å¼è°ƒç”¨

```python
from langchain_core.runnables import RunnablePassthrough

def debug_print(x):
    """è°ƒè¯•æ‰“å°"""
    print(f"ğŸ” ä¸­é—´ç»“æœ: {x}")
    return x

# åœ¨é“¾ä¸­æ’å…¥è°ƒè¯•èŠ‚ç‚¹
debug_runnable = RunnableLambda(debug_print)

chain_with_debug = (
    prompt 
    | debug_runnable  # è°ƒè¯•ç‚¹ 1
    | model 
    | debug_runnable  # è°ƒè¯•ç‚¹ 2
    | parser
)

result = chain_with_debug.invoke({"topic": "çŒ«"})
```

### 3.4 ç»„ä»¶åŒ–è®¾è®¡æ€æƒ³

#### å•ä¸€èŒè´£åŸåˆ™

æ¯ä¸ªç»„ä»¶åªåšä¸€ä»¶äº‹ï¼š

```python
# âœ… å¥½çš„è®¾è®¡ - æ¯ä¸ªç»„ä»¶èŒè´£å•ä¸€
from langchain.tools import tool

@tool
def get_weather(city: str) -> str:
    """åªè´Ÿè´£è·å–å¤©æ°”"""
    return f"{city}çš„å¤©æ°”"

@tool
def translate_text(text: str, target_lang: str) -> str:
    """åªè´Ÿè´£ç¿»è¯‘"""
    return f"ç¿»è¯‘ç»“æœ"

# âŒ ä¸å¥½çš„è®¾è®¡ - ä¸€ä¸ªå·¥å…·åšå¤ªå¤šäº‹
@tool
def do_everything(city: str, translate: bool, lang: str) -> str:
    """åˆæŸ¥å¤©æ°”åˆç¿»è¯‘ï¼ŒèŒè´£ä¸æ¸…"""
    weather = f"{city}çš„å¤©æ°”"
    if translate:
        return f"ç¿»è¯‘: {weather}"
    return weather
```

#### å¯ç»„åˆæ€§

```python
# å°ç»„ä»¶å¯ä»¥ç»„åˆæˆå¤§ç»„ä»¶

# åŸºç¡€ç»„ä»¶
weather_tool = get_weather
translation_tool = translate_text

# ç»„åˆæˆ Agent
weather_agent = create_agent(
    model="gpt-4",
    tools=[weather_tool]
)

multilingual_weather_agent = create_agent(
    model="gpt-4",
    tools=[weather_tool, translation_tool]
)
```

#### å¯å¤ç”¨æ€§

```python
# å®šä¹‰ä¸€æ¬¡ï¼Œåˆ°å¤„ä½¿ç”¨

# å®šä¹‰å¯å¤ç”¨çš„æç¤ºæ¨¡æ¿
HELPFUL_ASSISTANT_PROMPT = ChatPromptTemplate.from_messages([
    ("system", "ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„åŠ©æ‰‹"),
    ("user", "{input}")
])

# åœ¨å¤šä¸ªé“¾ä¸­å¤ç”¨
chain1 = HELPFUL_ASSISTANT_PROMPT | model1 | parser
chain2 = HELPFUL_ASSISTANT_PROMPT | model2 | parser
chain3 = HELPFUL_ASSISTANT_PROMPT | model3 | parser
```

---

## ç¬¬å››éƒ¨åˆ†ï¼šæ¨¡å‹é›†æˆå®Œå…¨æŒ‡å—

### 4.1 åˆå§‹åŒ–æ¨¡å‹çš„ä¸‰ç§æ–¹å¼

#### æ–¹å¼ 1ï¼šä½¿ç”¨å­—ç¬¦ä¸²æ ‡è¯†ç¬¦ï¼ˆæœ€ç®€å•ï¼‰

```python
from langchain.agents import create_agent

# LangChain ä¼šè‡ªåŠ¨æ¨æ–­æä¾›å•†
agent = create_agent(
    "gpt-4",  # è‡ªåŠ¨è¯†åˆ«ä¸º OpenAI
    tools=[]
)

# å¯¹äºå…¶ä»–æä¾›å•†ï¼Œä½¿ç”¨å‰ç¼€
agent2 = create_agent(
    "google_genai:gemini-2.5-flash-lite",  # Google Gemini
    tools=[]
)
```

**ä¼˜ç‚¹**ï¼š
- ä»£ç ç®€æ´
- å¿«é€ŸåŸå‹å¼€å‘
- æ˜“äºåˆ‡æ¢æ¨¡å‹

**ç¼ºç‚¹**ï¼š
- æ— æ³•ç²¾ç»†æ§åˆ¶å‚æ•°
- éœ€è¦éµå¾ªå‘½åè§„èŒƒ

#### æ–¹å¼ 2ï¼šä½¿ç”¨ init_chat_modelï¼ˆæ¨èï¼‰

```python
from langchain.chat_models import init_chat_model

# åŸºç¡€åˆå§‹åŒ–
model = init_chat_model("gpt-4")

# å¸¦å‚æ•°åˆå§‹åŒ–
model = init_chat_model(
    "claude-sonnet-4-5-20250929",
    temperature=0.7,      # åˆ›é€ æ€§
    timeout=30,           # è¶…æ—¶æ—¶é—´
    max_tokens=1000,      # æœ€å¤§ token æ•°
    max_retries=3         # é‡è¯•æ¬¡æ•°
)

# æŒ‡å®šæä¾›å•†
model = init_chat_model(
    model="gpt-4",
    model_provider="openai"
)
```

**ä¼˜ç‚¹**ï¼š
- ç»Ÿä¸€çš„æ¥å£
- æ”¯æŒå‚æ•°é…ç½®
- ä¾¿äºåˆ‡æ¢æä¾›å•†

#### æ–¹å¼ 3ï¼šç›´æ¥å®ä¾‹åŒ–ï¼ˆæœ€çµæ´»ï¼‰

```python
from langchain_openai import ChatOpenAI

model = ChatOpenAI(
    model="gpt-4",
    temperature=0.7,
    max_tokens=2000,
    timeout=60,
    max_retries=3,
    api_key="sk-...",  # å¯ä»¥ç›´æ¥ä¼ å…¥
    organization="org-...",
    base_url="https://api.openai.com/v1"
)
```

**ä¼˜ç‚¹**ï¼š
- å®Œå…¨æ§åˆ¶æ‰€æœ‰å‚æ•°
- æ”¯æŒé«˜çº§é…ç½®
- ç±»å‹æç¤ºå®Œæ•´

### 4.2 æ¨¡å‹å‚æ•°è¯¦è§£

#### temperatureï¼ˆæ¸©åº¦ï¼‰

æ§åˆ¶è¾“å‡ºçš„éšæœºæ€§å’Œåˆ›é€ æ€§ï¼š

```python
from langchain_openai import ChatOpenAI

# ä½æ¸©åº¦ - æ›´ç¡®å®šæ€§ï¼Œæ›´ä¸€è‡´
formal_model = ChatOpenAI(
    model="gpt-4",
    temperature=0.1  # é€‚åˆï¼šä»£ç ç”Ÿæˆã€æ•°æ®æå–ã€ç²¾ç¡®ä»»åŠ¡
)

response = formal_model.invoke("2 + 2 = ?")
# è¾“å‡º: "4"ï¼ˆæ¯æ¬¡éƒ½ç›¸åŒï¼‰

# ä¸­æ¸©åº¦ - å¹³è¡¡
balanced_model = ChatOpenAI(
    model="gpt-4",
    temperature=0.7  # é€‚åˆï¼šå¯¹è¯ã€é—®ç­”ã€ä¸€èˆ¬ä»»åŠ¡
)

# é«˜æ¸©åº¦ - æ›´åˆ›é€ æ€§ï¼Œæ›´å¤šæ ·
creative_model = ChatOpenAI(
    model="gpt-4",
    temperature=1.5  # é€‚åˆï¼šåˆ›æ„å†™ä½œã€å¤´è„‘é£æš´
)

response = creative_model.invoke("å†™ä¸€ä¸ªç§‘å¹»æ•…äº‹å¼€å¤´")
# æ¯æ¬¡è¾“å‡ºéƒ½ä¸åŒï¼Œæ›´æœ‰åˆ›æ„
```

| Temperature | ç‰¹ç‚¹ | é€‚ç”¨åœºæ™¯ |
|------------|------|---------|
| 0.0 - 0.3 | é«˜ç¡®å®šæ€§ã€ä½åˆ›é€ æ€§ | ä»£ç ç”Ÿæˆã€æ•°æ®æå–ã€åˆ†ç±» |
| 0.4 - 0.7 | å¹³è¡¡æ€§ | å¯¹è¯ã€é—®ç­”ã€ç¿»è¯‘ |
| 0.8 - 1.5 | é«˜åˆ›é€ æ€§ã€ä½ç¡®å®šæ€§ | åˆ›æ„å†™ä½œã€å¤´è„‘é£æš´ |

#### max_tokensï¼ˆæœ€å¤§ token æ•°ï¼‰

```python
# é™åˆ¶è¾“å‡ºé•¿åº¦
short_model = ChatOpenAI(
    model="gpt-4",
    max_tokens=100  # é€‚åˆï¼šç®€çŸ­å›ç­”ã€æ‘˜è¦
)

long_model = ChatOpenAI(
    model="gpt-4",
    max_tokens=4000  # é€‚åˆï¼šé•¿æ–‡æœ¬ç”Ÿæˆã€è¯¦ç»†è§£é‡Š
)

# ç¤ºä¾‹
response = short_model.invoke("è§£é‡Šé‡å­è®¡ç®—")
print(len(response.content))  # å¤§çº¦ 100 ä¸ª token

response = long_model.invoke("è¯¦ç»†è§£é‡Šé‡å­è®¡ç®—çš„å†å²å’Œåº”ç”¨")
print(len(response.content))  # å¯ä»¥è¾¾åˆ° 4000 ä¸ª token
```

#### timeoutï¼ˆè¶…æ—¶æ—¶é—´ï¼‰

```python
# è®¾ç½®è¶…æ—¶é¿å…æ— é™ç­‰å¾…
model = ChatOpenAI(
    model="gpt-4",
    timeout=30  # 30 ç§’è¶…æ—¶
)

try:
    response = model.invoke("éå¸¸å¤æ‚çš„é—®é¢˜...")
except Exception as e:
    print(f"è¯·æ±‚è¶…æ—¶: {e}")
```

#### max_retriesï¼ˆæœ€å¤§é‡è¯•æ¬¡æ•°ï¼‰

```python
# è‡ªåŠ¨é‡è¯•å¤±è´¥çš„è¯·æ±‚
robust_model = ChatOpenAI(
    model="gpt-4",
    max_retries=5  # å¤±è´¥åé‡è¯• 5 æ¬¡
)

# é€‚åˆç”Ÿäº§ç¯å¢ƒï¼Œå¢åŠ å¯é æ€§
```

### 4.3 å®æˆ˜ï¼šæ„å»ºå¯é…ç½®çš„æ¨¡å‹ç®¡ç†å™¨

```python
from typing import Literal
from langchain_openai import ChatOpenAI
from langchain_anthropic import ChatAnthropic

class ModelManager:
    """æ¨¡å‹ç®¡ç†å™¨ - ç»Ÿä¸€ç®¡ç†ä¸åŒçš„æ¨¡å‹é…ç½®"""
    
    def __init__(self):
        self.models = {}
    
    def register_model(
        self,
        name: str,
        provider: Literal["openai", "anthropic"],
        model_name: str,
        **kwargs
    ):
        """æ³¨å†Œæ¨¡å‹é…ç½®"""
        if provider == "openai":
            self.models[name] = ChatOpenAI(
                model=model_name,
                **kwargs
            )
        elif provider == "anthropic":
            self.models[name] = ChatAnthropic(
                model=model_name,
                **kwargs
            )
    
    def get_model(self, name: str):
        """è·å–æ¨¡å‹"""
        return self.models.get(name)
    
    def list_models(self):
        """åˆ—å‡ºæ‰€æœ‰æ¨¡å‹"""
        return list(self.models.keys())

# ä½¿ç”¨ç¤ºä¾‹
manager = ModelManager()

# æ³¨å†Œä¸åŒç”¨é€”çš„æ¨¡å‹
manager.register_model(
    name="fast",
    provider="openai",
    model_name="gpt-3.5-turbo",
    temperature=0.5,
    max_tokens=500
)

manager.register_model(
    name="smart",
    provider="openai",
    model_name="gpt-4",
    temperature=0.7,
    max_tokens=2000
)

manager.register_model(
    name="creative",
    provider="anthropic",
    model_name="claude-sonnet-4-5-20250929",
    temperature=1.0,
    max_tokens=3000
)

# æ ¹æ®ä»»åŠ¡é€‰æ‹©æ¨¡å‹
fast_model = manager.get_model("fast")
response = fast_model.invoke("å¿«é€Ÿé—®ç­”")

smart_model = manager.get_model("smart")
response = smart_model.invoke("å¤æ‚æ¨ç†")

creative_model = manager.get_model("creative")
response = creative_model.invoke("åˆ›æ„å†™ä½œ")
```

### 4.4 æ¨¡å‹è°ƒç”¨é«˜çº§æŠ€å·§

#### æŠ€å·§ 1ï¼šä½¿ç”¨æ¶ˆæ¯ç±»å‹

```python
from langchain_core.messages import (
    HumanMessage,
    AIMessage,
    SystemMessage
)
from langchain_openai import ChatOpenAI

model = ChatOpenAI()

# ç»“æ„åŒ–çš„å¯¹è¯å†å²
messages = [
    SystemMessage(content="ä½ æ˜¯ä¸€ä¸ª Python ç¼–ç¨‹ä¸“å®¶"),
    HumanMessage(content="ä»€ä¹ˆæ˜¯è£…é¥°å™¨ï¼Ÿ"),
    AIMessage(content="è£…é¥°å™¨æ˜¯ä¿®æ”¹å‡½æ•°è¡Œä¸ºçš„å‡½æ•°..."),
    HumanMessage(content="ç»™æˆ‘ä¸€ä¸ªç¤ºä¾‹")
]

response = model.invoke(messages)
print(response.content)
```

#### æŠ€å·§ 2ï¼šæµå¼è¾“å‡ºä¼˜åŒ–

```python
import sys

def stream_response(query: str):
    """æµå¼è¾“å‡ºå¹¶æ˜¾ç¤ºè¿›åº¦"""
    model = ChatOpenAI(model="gpt-4")
    
    print("AI: ", end="", flush=True)
    full_response = ""
    
    for chunk in model.stream(query):
        content = chunk.content
        full_response += content
        print(content, end="", flush=True)
        sys.stdout.flush()
    
    print()  # æ¢è¡Œ
    return full_response

# ä½¿ç”¨
response = stream_response("è§£é‡Šæœºå™¨å­¦ä¹ ")
```

#### æŠ€å·§ 3ï¼šé”™è¯¯å¤„ç†

```python
from langchain_openai import ChatOpenAI
from langchain_core.exceptions import LangChainException
import time

def invoke_with_retry(model, query, max_retries=3):
    """å¸¦é‡è¯•çš„æ¨¡å‹è°ƒç”¨"""
    for attempt in range(max_retries):
        try:
            return model.invoke(query)
        except LangChainException as e:
            if attempt == max_retries - 1:
                raise
            print(f"å°è¯• {attempt + 1} å¤±è´¥ï¼Œé‡è¯•ä¸­...")
            time.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿

# ä½¿ç”¨
model = ChatOpenAI(model="gpt-4")
response = invoke_with_retry(model, "ä½ å¥½")
```

---

## ç¬¬äº”éƒ¨åˆ†ï¼šåˆ›å»ºç¬¬ä¸€ä¸ª Agent

### 5.1 Agent åŸºæœ¬æ¦‚å¿µ

#### ä»€ä¹ˆæ˜¯ Agentï¼Ÿ

Agent æ˜¯ä¸€ä¸ªèƒ½å¤Ÿï¼š
1. **ç†è§£ä»»åŠ¡**ï¼šè§£æç”¨æˆ·æ„å›¾
2. **é€‰æ‹©å·¥å…·**ï¼šå†³å®šä½¿ç”¨å“ªäº›å·¥å…·
3. **æ‰§è¡Œæ“ä½œ**ï¼šè°ƒç”¨å·¥å…·è·å–ç»“æœ
4. **ç»¼åˆå›ç­”**ï¼šåŸºäºç»“æœç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ

```
ç”¨æˆ·é—®é¢˜
    â†“
ã€Agent æ¨ç†ã€‘
    â†“
é€‰æ‹©å·¥å…· A â†’ æ‰§è¡Œ â†’ è·å–ç»“æœ A
    â†“
ã€Agent æ¨ç†ã€‘
    â†“
é€‰æ‹©å·¥å…· B â†’ æ‰§è¡Œ â†’ è·å–ç»“æœ B
    â†“
ã€Agent æ¨ç†ã€‘
    â†“
ç»¼åˆ A å’Œ B â†’ ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ
```

### 5.2 åˆ›å»ºæœ€ç®€å•çš„ Agent

```python
from langchain.agents import create_agent
from langchain.tools import tool

# æ­¥éª¤ 1: å®šä¹‰å·¥å…·
@tool
def calculator(expression: str) -> str:
    """è®¡ç®—æ•°å­¦è¡¨è¾¾å¼ã€‚
    
    Args:
        expression: æ•°å­¦è¡¨è¾¾å¼ï¼Œå¦‚ "2 + 3 * 4"
    """
    try:
        result = eval(expression)
        return f"ç»“æœæ˜¯: {result}"
    except Exception as e:
        return f"è®¡ç®—é”™è¯¯: {e}"

# æ­¥éª¤ 2: åˆ›å»º Agent
agent = create_agent(
    model="gpt-4",
    tools=[calculator],
    system_prompt="ä½ æ˜¯ä¸€ä¸ªæ•°å­¦åŠ©æ‰‹ï¼Œå¸®åŠ©ç”¨æˆ·è¿›è¡Œè®¡ç®—"
)

# æ­¥éª¤ 3: ä½¿ç”¨ Agent
response = agent.invoke({
    "messages": [{"role": "user", "content": "è®¡ç®— 123 * 456"}]
})

print(response["messages"][-1].content)
```

### 5.3 å¤šå·¥å…· Agent

```python
from langchain.agents import create_agent
from langchain.tools import tool
from datetime import datetime

# å·¥å…· 1: è·å–å½“å‰æ—¶é—´
@tool
def get_current_time() -> str:
    """è·å–å½“å‰æ—¶é—´"""
    return datetime.now().strftime("%Y-%m-%d %H:%M:%S")

# å·¥å…· 2: å¤©æ°”æŸ¥è¯¢
@tool
def get_weather(city: str) -> str:
    """è·å–åŸå¸‚å¤©æ°”
    
    Args:
        city: åŸå¸‚åç§°
    """
    # æ¨¡æ‹Ÿå¤©æ°”æ•°æ®
    weather_data = {
        "åŒ—äº¬": "æ™´å¤©ï¼Œ15Â°C",
        "ä¸Šæµ·": "å¤šäº‘ï¼Œ18Â°C",
        "æ·±åœ³": "é˜´å¤©ï¼Œ22Â°C"
    }
    return weather_data.get(city, f"æœªæ‰¾åˆ° {city} çš„å¤©æ°”æ•°æ®")

# å·¥å…· 3: è´§å¸è½¬æ¢
@tool
def convert_currency(amount: float, from_currency: str, to_currency: str) -> str:
    """è´§å¸è½¬æ¢
    
    Args:
        amount: é‡‘é¢
        from_currency: æºè´§å¸
        to_currency: ç›®æ ‡è´§å¸
    """
    # æ¨¡æ‹Ÿæ±‡ç‡
    rates = {
        ("USD", "CNY"): 7.2,
        ("CNY", "USD"): 0.14,
        ("USD", "EUR"): 0.92,
    }
    rate = rates.get((from_currency, to_currency), 1.0)
    result = amount * rate
    return f"{amount} {from_currency} = {result:.2f} {to_currency}"

# åˆ›å»ºå¤šåŠŸèƒ½ Agent
assistant = create_agent(
    model="gpt-4",
    tools=[get_current_time, get_weather, convert_currency],
    system_prompt="""ä½ æ˜¯ä¸€ä¸ªå¤šåŠŸèƒ½åŠ©æ‰‹ï¼Œå¯ä»¥ï¼š
    1. æŸ¥è¯¢å½“å‰æ—¶é—´
    2. æŸ¥è¯¢å¤©æ°”
    3. è¿›è¡Œè´§å¸è½¬æ¢
    
    æ ¹æ®ç”¨æˆ·é—®é¢˜ï¼Œé€‰æ‹©åˆé€‚çš„å·¥å…·æ¥å›ç­”ã€‚
    """
)

# æµ‹è¯• Agent
test_queries = [
    "ç°åœ¨å‡ ç‚¹äº†ï¼Ÿ",
    "åŒ—äº¬çš„å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ",
    "100 ç¾å…ƒæ˜¯å¤šå°‘äººæ°‘å¸ï¼Ÿ"
]

for query in test_queries:
    print(f"\nç”¨æˆ·: {query}")
    response = assistant.invoke({
        "messages": [{"role": "user", "content": query}]
    })
    print(f"AI: {response['messages'][-1].content}")
```

### 5.4 Agent æ‰§è¡Œæµç¨‹è¯¦è§£

è®©æˆ‘ä»¬æ·±å…¥ç†è§£ Agent çš„æ‰§è¡Œè¿‡ç¨‹ï¼š

```python
from langchain.agents import create_agent
from langchain.tools import tool

@tool
def search(query: str) -> str:
    """æœç´¢ä¿¡æ¯"""
    return f"æœç´¢ç»“æœï¼šå…³äº {query} çš„ä¿¡æ¯..."

@tool
def calculate(expression: str) -> str:
    """è®¡ç®—æ•°å­¦è¡¨è¾¾å¼"""
    return f"è®¡ç®—ç»“æœï¼š{eval(expression)}"

# åˆ›å»º Agent
agent = create_agent(
    model="gpt-4",
    tools=[search, calculate]
)

# æ‰§è¡Œå¹¶è§‚å¯Ÿæµç¨‹
response = agent.invoke({
    "messages": [{"role": "user", "content": "æœç´¢é‡å­è®¡ç®—ï¼Œç„¶åè®¡ç®— 2^10"}]
})

# Agent çš„æ‰§è¡Œè¿‡ç¨‹ï¼š
# 
# ã€ç¬¬1è½®ã€‘
# Agent æ€è€ƒï¼šç”¨æˆ·è¦æˆ‘æœç´¢é‡å­è®¡ç®—
# â†“
# è°ƒç”¨å·¥å…·ï¼šsearch("é‡å­è®¡ç®—")
# â†“
# å·¥å…·è¿”å›ï¼š"æœç´¢ç»“æœï¼šå…³äºé‡å­è®¡ç®—çš„ä¿¡æ¯..."
# 
# ã€ç¬¬2è½®ã€‘
# Agent æ€è€ƒï¼šç°åœ¨éœ€è¦è®¡ç®— 2^10
# â†“
# è°ƒç”¨å·¥å…·ï¼šcalculate("2**10")
# â†“
# å·¥å…·è¿”å›ï¼š"è®¡ç®—ç»“æœï¼š1024"
# 
# ã€ç¬¬3è½®ã€‘
# Agent æ€è€ƒï¼šå·²ç»å®Œæˆä¸¤ä¸ªä»»åŠ¡ï¼Œå¯ä»¥å›ç­”ç”¨æˆ·äº†
# â†“
# ç”Ÿæˆå›ç­”ï¼š"æˆ‘å·²ç»æœç´¢äº†é‡å­è®¡ç®—çš„ä¿¡æ¯ï¼Œå¹¶è®¡ç®—å‡º 2^10 = 1024"

print(response["messages"][-1].content)
```

### 5.5 å®æˆ˜é¡¹ç›®ï¼šä¸ªäººåŠ©ç† Agent

åˆ›å»ºä¸€ä¸ªå®Œæ•´çš„ä¸ªäººåŠ©ç† Agentï¼š

```python
from langchain.agents import create_agent
from langchain.tools import tool
from langgraph.checkpoint.memory import InMemorySaver
from datetime import datetime
import random

# === å·¥å…·å®šä¹‰ ===

@tool
def get_time() -> str:
    """è·å–å½“å‰æ—¶é—´"""
    return datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S")

@tool
def set_reminder(task: str, time: str) -> str:
    """è®¾ç½®æé†’
    
    Args:
        task: æé†’äº‹é¡¹
        time: æé†’æ—¶é—´
    """
    return f"âœ… å·²è®¾ç½®æé†’ï¼š{task}ï¼Œæ—¶é—´ï¼š{time}"

@tool
def get_weather(city: str) -> str:
    """è·å–å¤©æ°”"""
    weathers = ["æ™´å¤©", "å¤šäº‘", "å°é›¨", "é˜´å¤©"]
    temp = random.randint(15, 30)
    return f"{city}çš„å¤©æ°”ï¼š{random.choice(weathers)}ï¼Œæ¸©åº¦{temp}Â°C"

@tool
def search_info(query: str) -> str:
    """æœç´¢ä¿¡æ¯"""
    return f"å…³äº'{query}'çš„æœç´¢ç»“æœï¼šè¿™æ˜¯ä¸€ä¸ªé‡è¦çš„è¯é¢˜..."

@tool
def calculate(expression: str) -> str:
    """è®¡ç®—å™¨"""
    try:
        result = eval(expression)
        return f"è®¡ç®—ç»“æœï¼š{result}"
    except:
        return "è®¡ç®—å‡ºé”™ï¼Œè¯·æ£€æŸ¥è¡¨è¾¾å¼"

@tool
def translate(text: str, target_lang: str) -> str:
    """ç¿»è¯‘æ–‡æœ¬
    
    Args:
        text: è¦ç¿»è¯‘çš„æ–‡æœ¬
        target_lang: ç›®æ ‡è¯­è¨€ï¼ˆå¦‚ï¼šè‹±æ–‡ã€æ—¥æ–‡ï¼‰
    """
    return f"[ç¿»è¯‘ç»“æœ] {text} â†’ {target_lang}"

# === åˆ›å»ºä¸ªäººåŠ©ç† ===

personal_assistant = create_agent(
    model="gpt-4",
    tools=[
        get_time,
        set_reminder,
        get_weather,
        search_info,
        calculate,
        translate
    ],
    system_prompt="""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä¸ªäººåŠ©ç†ï¼Œåå­—å«å°æ™ºã€‚

ä½ çš„èƒ½åŠ›ï¼š
1. ğŸ“… æ—¶é—´ç®¡ç†ï¼šæŸ¥è¯¢æ—¶é—´ã€è®¾ç½®æé†’
2. ğŸŒ¤ï¸  å¤©æ°”æŸ¥è¯¢ï¼šæŸ¥è¯¢å„åŸå¸‚å¤©æ°”
3. ğŸ” ä¿¡æ¯æœç´¢ï¼šæœç´¢å„ç±»ä¿¡æ¯
4. ğŸ§® æ•°å­¦è®¡ç®—ï¼šè¿›è¡Œæ•°å­¦è¿ç®—
5. ğŸŒ æ–‡æœ¬ç¿»è¯‘ï¼šç¿»è¯‘å¤šç§è¯­è¨€

å·¥ä½œåŸåˆ™ï¼š
- å‹å¥½ã€ä¸“ä¸šã€é«˜æ•ˆ
- ä¸»åŠ¨ç†è§£ç”¨æˆ·éœ€æ±‚
- æä¾›å‡†ç¡®çš„ä¿¡æ¯
- å¿…è¦æ—¶ä½¿ç”¨å¤šä¸ªå·¥å…·å®Œæˆä»»åŠ¡
    """,
    checkpointer=InMemorySaver()  # è®°å¿†ç³»ç»Ÿ
)

# === äº¤äº’å¼æµ‹è¯• ===

def chat_with_assistant():
    """ä¸åŠ©ç†äº¤äº’"""
    print("=" * 60)
    print("ä¸ªäººåŠ©ç†å°æ™ºå·²å°±ç»ªï¼è¾“å…¥ 'quit' é€€å‡º")
    print("=" * 60)
    
    conversation_id = "user_001"
    
    while True:
        user_input = input("\nä½ : ").strip()
        
        if user_input.lower() in ['quit', 'exit', 'é€€å‡º']:
            print("å°æ™º: å†è§ï¼ç¥ä½ æœ‰ç¾å¥½çš„ä¸€å¤©ï¼")
            break
        
        if not user_input:
            continue
        
        # è°ƒç”¨ Agent
        response = personal_assistant.invoke(
            {"messages": [{"role": "user", "content": user_input}]},
            {"configurable": {"thread_id": conversation_id}}
        )
        
        # æ˜¾ç¤ºå›ç­”
        ai_response = response["messages"][-1].content
        print(f"\nå°æ™º: {ai_response}")

# è¿è¡Œï¼ˆæ³¨é‡Šæ‰ä»¥é˜²æ­¢é˜»å¡ï¼‰
# chat_with_assistant()

# æµ‹è¯•ç¤ºä¾‹
test_conversations = [
    "ç°åœ¨å‡ ç‚¹äº†ï¼Ÿ",
    "æ˜å¤©ä¸Šåˆ9ç‚¹æé†’æˆ‘å¼€ä¼š",
    "åŒ—äº¬çš„å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ",
    "æœç´¢é‡å­è®¡ç®—",
    "è®¡ç®— (123 + 456) * 2",
    "æŠŠ 'Hello World' ç¿»è¯‘æˆä¸­æ–‡"
]

print("\n" + "=" * 60)
print("æµ‹è¯•å¯¹è¯")
print("=" * 60)

for query in test_conversations:
    print(f"\nç”¨æˆ·: {query}")
    response = personal_assistant.invoke(
        {"messages": [{"role": "user", "content": query}]},
        {"configurable": {"thread_id": "test_session"}}
    )
    print(f"å°æ™º: {response['messages'][-1].content}")
```

### 5.6 ç¬¬ä¸€é˜¶æ®µæ€»ç»“ä¸ç»ƒä¹ 

#### ä½ å·²ç»å­¦ä¼šäº†ï¼š

âœ… **LangChain æ¦‚è¿°**
- LangChain çš„æ ¸å¿ƒä»·å€¼
- è§£å†³çš„é—®é¢˜å’Œåº”ç”¨åœºæ™¯
- ç”Ÿæ€ç³»ç»Ÿç»„æˆ

âœ… **å®‰è£…ä¸é…ç½®**
- å®‰è£…å„ç§ç»„ä»¶
- é…ç½®æ¨¡å‹æä¾›å•†
- ç¯å¢ƒå˜é‡ç®¡ç†

âœ… **æ ¸å¿ƒæ¶æ„**
- Runnable æ¥å£
- LCEL è¡¨è¾¾å¼è¯­è¨€
- ç»„ä»¶åŒ–è®¾è®¡

âœ… **æ¨¡å‹é›†æˆ**
- ä¸‰ç§åˆå§‹åŒ–æ–¹å¼
- å‚æ•°é…ç½®å’Œä¼˜åŒ–
- é«˜çº§è°ƒç”¨æŠ€å·§

âœ… **Agent åˆ›å»º**
- å·¥å…·å®šä¹‰
- Agent æ„å»º
- æ‰§è¡Œæµç¨‹ç†è§£

#### ç»ƒä¹ é¡¹ç›®

**ç»ƒä¹  1ï¼šæ„å»ºå¤©æ°”åŠ©æ‰‹**
```python
# è¦æ±‚ï¼š
# 1. åˆ›å»ºè·å–å¤©æ°”çš„å·¥å…·
# 2. åˆ›å»ºè·å–æ—¶é—´çš„å·¥å…·
# 3. æ„å»ºä¸€ä¸ª Agent å¯ä»¥å›ç­”ï¼š
#    - "ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ"
#    - "ç°åœ¨å‡ ç‚¹äº†ï¼Ÿ"
#    - "æ˜å¤©çš„å¤©æ°”é€‚åˆæˆ·å¤–è¿åŠ¨å—ï¼Ÿ"
```

**ç»ƒä¹  2ï¼šåˆ›å»ºå­¦ä¹ åŠ©æ‰‹**
```python
# è¦æ±‚ï¼š
# 1. åˆ›å»ºæœç´¢å·¥å…·ï¼ˆæ¨¡æ‹Ÿæœç´¢åŠŸèƒ½ï¼‰
# 2. åˆ›å»ºæ€»ç»“å·¥å…·ï¼ˆæå–å…³é”®ä¿¡æ¯ï¼‰
# 3. åˆ›å»ºç¿»è¯‘å·¥å…·
# 4. Agent èƒ½å¤Ÿï¼š
#    - æœç´¢å­¦ä¹ èµ„æ–™
#    - æ€»ç»“é‡ç‚¹
#    - ç¿»è¯‘å†…å®¹
```

**ç»ƒä¹  3ï¼šæ„å»ºä»»åŠ¡ç®¡ç† Agent**
```python
# è¦æ±‚ï¼š
# 1. åˆ›å»ºæ·»åŠ ä»»åŠ¡å·¥å…·
# 2. åˆ›å»ºæŸ¥çœ‹ä»»åŠ¡å·¥å…·
# 3. åˆ›å»ºå®Œæˆä»»åŠ¡å·¥å…·
# 4. Agent èƒ½å¤Ÿç®¡ç†å¾…åŠäº‹é¡¹
```

---

## ğŸ“ ç¬¬ä¸€é˜¶æ®µå®Œæˆï¼

æ­å–œä½ å®Œæˆäº† LangChain ç¬¬ä¸€é˜¶æ®µçš„å­¦ä¹ ï¼ä½ ç°åœ¨å·²ç»æŒæ¡äº†ï¼š

- LangChain çš„åŸºç¡€æ¦‚å¿µå’Œæ ¸å¿ƒä»·å€¼
- å¦‚ä½•å®‰è£…å’Œé…ç½® LangChain
- Runnable æ¥å£å’Œ LCEL çš„ä½¿ç”¨
- æ¨¡å‹åˆå§‹åŒ–å’Œå‚æ•°è°ƒä¼˜
- åˆ›å»ºå’Œä½¿ç”¨ Agent

### ğŸ“Œ ä¸‹ä¸€é˜¶æ®µé¢„å‘Š

**ç¬¬äºŒé˜¶æ®µï¼šLangGraph ä¸å·¥ä½œæµç¼–æ’**

ä½ å°†å­¦ä¹ ï¼š
1. ğŸ”„ LangGraph æ·±å…¥ç†è§£
2. ğŸ“Š StateGraph çŠ¶æ€ç®¡ç†
3. ğŸ”€ æ¡ä»¶è·¯ç”±å’Œå¹¶è¡Œæ‰§è¡Œ
4. ğŸ—ï¸ å¤æ‚å·¥ä½œæµæ„å»º
5. ğŸ’¾ Checkpointer å’ŒæŒä¹…åŒ–

å‡†å¤‡å¥½äº†å—ï¼Ÿè®©æˆ‘ä»¬è¿›å…¥ä¸‹ä¸€é˜¶æ®µçš„å­¦ä¹ ï¼

