# ç¬¬ä¸‰é˜¶æ®µï¼šé«˜çº§ç‰¹æ€§ä¸å®æˆ˜é¡¹ç›®ï¼ˆè¯¦ç»†è®²è§£ï¼‰

> **å­¦ä¹ ç›®æ ‡**ï¼šæŒæ¡ LangChain é«˜çº§ç‰¹æ€§ã€æ„å»ºç”Ÿäº§çº§åº”ç”¨ã€å®ç°å®Œæ•´çš„ AI é¡¹ç›®
> 
> **é¢„è®¡å­¦ä¹ æ—¶é—´**ï¼š3-4å‘¨
> 
> **å‰ç½®è¦æ±‚**ï¼šå®Œæˆå‰ä¸¤é˜¶æ®µå­¦ä¹ ã€ç†è§£ LangGraph å·¥ä½œæµã€ç†Ÿæ‚‰å¼‚æ­¥ç¼–ç¨‹

---

## ğŸ“š ç¬¬ä¸‰é˜¶æ®µå­¦ä¹ å¤§çº²

1. [Tools å·¥å…·ç³»ç»Ÿæ·±å…¥](#ç¬¬ä¸€éƒ¨åˆ†tools-å·¥å…·ç³»ç»Ÿæ·±å…¥)
   - å·¥å…·å®šä¹‰è¿›é˜¶
   - å¤æ‚å‚æ•°æ¨¡å¼
   - è¿è¡Œæ—¶ä¿¡æ¯è®¿é—®
   - å·¥å…·ç»„åˆç­–ç•¥

2. [RAG æ£€ç´¢å¢å¼ºç”Ÿæˆ](#ç¬¬äºŒéƒ¨åˆ†rag-æ£€ç´¢å¢å¼ºç”Ÿæˆ)
   - å‘é‡å­˜å‚¨è¯¦è§£
   - æ–‡æ¡£åŠ è½½ä¸åˆ†å‰²
   - æ£€ç´¢å™¨å®ç°
   - RAG ä¼˜åŒ–æŠ€å·§

3. [Streaming æµå¼å¤„ç†](#ç¬¬ä¸‰éƒ¨åˆ†streaming-æµå¼å¤„ç†)
   - æµå¼è¾“å‡ºåŸºç¡€
   - å¤šç§æµå¼æ¨¡å¼
   - è‡ªå®šä¹‰æµå¼äº‹ä»¶
   - æ€§èƒ½ä¼˜åŒ–

4. [Structured Output ç»“æ„åŒ–è¾“å‡º](#ç¬¬å››éƒ¨åˆ†structured-output-ç»“æ„åŒ–è¾“å‡º)
   - Pydantic æ¨¡å‹è¯¦è§£
   - ToolStrategy ç­–ç•¥
   - é”™è¯¯å¤„ç†æœºåˆ¶
   - é«˜çº§éªŒè¯æŠ€å·§

5. [Middleware ä¸ä¸­æ–­æœºåˆ¶](#ç¬¬äº”éƒ¨åˆ†middleware-ä¸ä¸­æ–­æœºåˆ¶)
   - äººå·¥ä»‹å…¥å¾ªç¯
   - Interrupt æœºåˆ¶
   - ä¸­é—´ä»¶æ¨¡å¼
   - é‡è¯•ç­–ç•¥

6. [å®Œæ•´å®æˆ˜é¡¹ç›®](#ç¬¬å…­éƒ¨åˆ†å®Œæ•´å®æˆ˜é¡¹ç›®)
   - æ™ºèƒ½æ–‡æ¡£é—®ç­”ç³»ç»Ÿ
   - å¤šAgentåä½œç³»ç»Ÿ
   - ç”Ÿäº§çº§éƒ¨ç½²æ–¹æ¡ˆ

---

## ç¬¬ä¸€éƒ¨åˆ†ï¼šTools å·¥å…·ç³»ç»Ÿæ·±å…¥

### 1.1 å·¥å…·å®šä¹‰è¿›é˜¶

#### åŸºç¡€å·¥å…·å®šä¹‰å›é¡¾

```python
from langchain.tools import tool

@tool
def simple_calculator(expression: str) -> str:
    """è®¡ç®—æ•°å­¦è¡¨è¾¾å¼
    
    Args:
        expression: æ•°å­¦è¡¨è¾¾å¼å­—ç¬¦ä¸²ï¼Œå¦‚ "2 + 3 * 4"
    """
    try:
        result = eval(expression)
        return f"è®¡ç®—ç»“æœ: {result}"
    except Exception as e:
        return f"è®¡ç®—é”™è¯¯: {e}"

# å·¥å…·çš„å…³é”®ç»„æˆï¼š
# 1. å‡½æ•°åï¼šsimple_calculator
# 2. æ–‡æ¡£å­—ç¬¦ä¸²ï¼šæè¿°å·¥å…·åŠŸèƒ½ï¼ˆLLM ä¼šè¯»å–ï¼‰
# 3. å‚æ•°ï¼šexpression (æœ‰ç±»å‹æ³¨è§£)
# 4. è¿”å›å€¼ï¼šå­—ç¬¦ä¸²ç±»å‹
```

#### ä½¿ç”¨ Pydantic å®šä¹‰å¤æ‚å‚æ•°

```python
from langchain.tools import tool
from pydantic import BaseModel, Field
from typing import Literal, Optional

# å®šä¹‰è¾“å…¥æ¨¡å¼
class WeatherInput(BaseModel):
    """å¤©æ°”æŸ¥è¯¢è¾“å…¥å‚æ•°"""
    location: str = Field(
        description="åŸå¸‚åç§°æˆ–åæ ‡",
        examples=["åŒ—äº¬", "ä¸Šæµ·", "40.7128,-74.0060"]
    )
    units: Literal["celsius", "fahrenheit"] = Field(
        default="celsius",
        description="æ¸©åº¦å•ä½"
    )
    include_forecast: bool = Field(
        default=False,
        description="æ˜¯å¦åŒ…å«5å¤©é¢„æŠ¥"
    )
    language: Optional[str] = Field(
        default="zh-CN",
        description="è¿”å›è¯­è¨€"
    )

@tool(args_schema=WeatherInput)
def get_weather(
    location: str,
    units: str = "celsius",
    include_forecast: bool = False,
    language: str = "zh-CN"
) -> str:
    """è·å–å¤©æ°”ä¿¡æ¯å’Œå¯é€‰çš„é¢„æŠ¥
    
    è¿™æ˜¯ä¸€ä¸ªé«˜çº§å¤©æ°”æŸ¥è¯¢å·¥å…·ï¼Œæ”¯æŒå¤šç§é…ç½®é€‰é¡¹ã€‚
    """
    temp = 22 if units == "celsius" else 72
    result = f"å½“å‰ {location} çš„å¤©æ°”ï¼š{temp}Â° {units[0].upper()}"
    
    if include_forecast:
        result += "\n\næœªæ¥5å¤©é¢„æŠ¥ï¼š"
        result += "\n- æ˜å¤©ï¼šæ™´å¤© 25Â°C"
        result += "\n- åå¤©ï¼šå¤šäº‘ 23Â°C"
        result += "\n- ç¬¬ä¸‰å¤©ï¼šå°é›¨ 20Â°C"
    
    return result

# ä½¿ç”¨å·¥å…·
from langchain.agents import create_agent

agent = create_agent(
    model="gpt-4",
    tools=[get_weather],
    system_prompt="ä½ æ˜¯ä¸€ä¸ªå¤©æ°”åŠ©æ‰‹"
)

result = agent.invoke({
    "messages": [{"role": "user", "content": "åŒ—äº¬çš„å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿç»™æˆ‘çœ‹çœ‹æœªæ¥å‡ å¤©çš„é¢„æŠ¥"}]
})

# LLM ä¼šè‡ªåŠ¨ï¼š
# 1. è§£æç”¨æˆ·æ„å›¾
# 2. è°ƒç”¨ get_weather(location="åŒ—äº¬", include_forecast=True)
# 3. ç”Ÿæˆå‹å¥½çš„å›å¤
```

### 1.2 å·¥å…·è¿”å›å€¼çš„é«˜çº§æ¨¡å¼

#### æ¨¡å¼ 1ï¼šè¿”å›å†…å®¹å’Œå…ƒæ•°æ®ï¼ˆArtifactï¼‰

```python
from langchain.tools import tool
from typing import Tuple, List, Dict

@tool(response_format="content_and_artifact")
def search_documents(query: str) -> Tuple[str, List[Dict]]:
    """æœç´¢æ–‡æ¡£å¹¶è¿”å›ç»“æœ
    
    è¿”å›ä¸¤éƒ¨åˆ†ï¼š
    1. æ ¼å¼åŒ–çš„æ–‡æœ¬ï¼ˆå‘é€ç»™ LLMï¼‰
    2. åŸå§‹æ–‡æ¡£æ•°æ®ï¼ˆä¾›åç»­å¤„ç†ï¼‰
    """
    # æ¨¡æ‹Ÿæ–‡æ¡£æœç´¢
    documents = [
        {
            "title": "LangChain æ•™ç¨‹",
            "content": "LangChain æ˜¯ä¸€ä¸ªå¼ºå¤§çš„æ¡†æ¶...",
            "score": 0.95,
            "metadata": {"author": "John", "date": "2024-01-01"}
        },
        {
            "title": "LangGraph æŒ‡å—",
            "content": "LangGraph ç”¨äºæ„å»ºå·¥ä½œæµ...",
            "score": 0.88,
            "metadata": {"author": "Jane", "date": "2024-01-15"}
        }
    ]
    
    # æ ¼å¼åŒ–æ–‡æœ¬ï¼ˆç»™ LLM çœ‹çš„ï¼‰
    formatted_text = f"æ‰¾åˆ° {len(documents)} ä¸ªç›¸å…³æ–‡æ¡£ï¼š\n\n"
    for i, doc in enumerate(documents, 1):
        formatted_text += f"{i}. {doc['title']}\n"
        formatted_text += f"   {doc['content'][:100]}...\n"
        formatted_text += f"   ç›¸å…³åº¦: {doc['score']:.2f}\n\n"
    
    # è¿”å›ï¼š(å†…å®¹, åŸå§‹æ•°æ®)
    return formatted_text, documents

# ä½¿ç”¨ç¤ºä¾‹
agent = create_agent(
    model="gpt-4",
    tools=[search_documents]
)

result = agent.invoke({
    "messages": [{"role": "user", "content": "æœç´¢ LangChain ç›¸å…³æ–‡æ¡£"}]
})

# è®¿é—®è¿”å›çš„å…ƒæ•°æ®
if hasattr(result, 'artifacts'):
    raw_documents = result.artifacts
    print(f"æ‰¾åˆ° {len(raw_documents)} ä¸ªæ–‡æ¡£")
    for doc in raw_documents:
        print(f"- {doc['title']} (è¯„åˆ†: {doc['score']})")
```

#### æ¨¡å¼ 2ï¼šæµå¼è¿”å›è¿›åº¦

```python
from langchain.tools import tool
from langgraph.config import get_stream_writer

@tool
def process_large_file(file_path: str) -> str:
    """å¤„ç†å¤§æ–‡ä»¶å¹¶å®æ—¶æŠ¥å‘Šè¿›åº¦"""
    writer = get_stream_writer()
    
    # æ¨¡æ‹Ÿå¤„ç†è¿‡ç¨‹
    total_steps = 100
    for i in range(1, total_steps + 1):
        # æ¯10æ­¥æŠ¥å‘Šä¸€æ¬¡
        if i % 10 == 0:
            writer(f"å¤„ç†è¿›åº¦: {i}/{total_steps} ({i}%)")
        
        # æ¨¡æ‹Ÿå¤„ç†
        import time
        time.sleep(0.01)
    
    writer("å¤„ç†å®Œæˆï¼")
    return f"æ–‡ä»¶ {file_path} å·²æˆåŠŸå¤„ç†"

# ä½¿ç”¨æµå¼æ¨¡å¼
agent = create_agent(
    model="gpt-4",
    tools=[process_large_file]
)

for chunk in agent.stream(
    {"messages": [{"role": "user", "content": "å¤„ç†æ–‡ä»¶ data.csv"}]},
    stream_mode="custom"
):
    print(chunk)  # å®æ—¶æ˜¾ç¤ºè¿›åº¦

# è¾“å‡ºï¼š
# å¤„ç†è¿›åº¦: 10/100 (10%)
# å¤„ç†è¿›åº¦: 20/100 (20%)
# ...
# å¤„ç†å®Œæˆï¼
```

### 1.3 è¿è¡Œæ—¶ä¿¡æ¯è®¿é—®

å·¥å…·å¯ä»¥è®¿é—®è¿è¡Œæ—¶ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼š

```python
from langchain.tools import tool
from langgraph.prebuilt import InjectedToolCallId, InjectedState
from typing import Annotated

@tool
def advanced_tool(
    query: str,
    tool_call_id: Annotated[str, InjectedToolCallId],
    state: Annotated[dict, InjectedState]
) -> str:
    """è®¿é—®è¿è¡Œæ—¶ä¿¡æ¯çš„é«˜çº§å·¥å…·
    
    Args:
        query: ç”¨æˆ·æŸ¥è¯¢
        tool_call_id: è‡ªåŠ¨æ³¨å…¥çš„å·¥å…·è°ƒç”¨ID
        state: è‡ªåŠ¨æ³¨å…¥çš„å½“å‰çŠ¶æ€
    """
    # è®¿é—®å·¥å…·è°ƒç”¨ ID
    print(f"å·¥å…·è°ƒç”¨ID: {tool_call_id}")
    
    # è®¿é—®çŠ¶æ€ä¿¡æ¯
    user_id = state.get("user_id", "unknown")
    conversation_history = state.get("messages", [])
    
    result = f"æŸ¥è¯¢: {query}\n"
    result += f"ç”¨æˆ·: {user_id}\n"
    result += f"å†å²æ¶ˆæ¯æ•°: {len(conversation_history)}\n"
    
    return result

# æ³¨æ„ï¼š
# - InjectedToolCallId å’Œ InjectedState ä¼šè‡ªåŠ¨æ³¨å…¥
# - LLM çœ‹ä¸åˆ°è¿™äº›å‚æ•°
# - åªæœ‰å®é™…çš„ä¸šåŠ¡å‚æ•°ï¼ˆqueryï¼‰ä¼šæš´éœ²ç»™ LLM
```

### 1.4 å·¥å…·ç»„åˆç­–ç•¥

#### ç­–ç•¥ 1ï¼šå·¥å…·é“¾ï¼ˆTool Chainï¼‰

```python
from langchain.tools import tool

@tool
def search_web(query: str) -> str:
    """æœç´¢ç½‘é¡µ"""
    return f"æœç´¢ç»“æœï¼šå…³äº {query} çš„ä¿¡æ¯..."

@tool
def summarize_text(text: str) -> str:
    """æ€»ç»“æ–‡æœ¬"""
    summary = text[:100] + "..."
    return f"æ‘˜è¦ï¼š{summary}"

@tool
def translate(text: str, target_lang: str) -> str:
    """ç¿»è¯‘æ–‡æœ¬"""
    return f"[{target_lang}] {text}"

# åˆ›å»ºå…·æœ‰å¤šä¸ªå·¥å…·çš„ Agent
research_agent = create_agent(
    model="gpt-4",
    tools=[search_web, summarize_text, translate],
    system_prompt="""ä½ æ˜¯ä¸€ä¸ªç ”ç©¶åŠ©æ‰‹ã€‚
    
    å·¥ä½œæµç¨‹ï¼š
    1. ä½¿ç”¨ search_web æœç´¢ä¿¡æ¯
    2. ä½¿ç”¨ summarize_text æ€»ç»“å†…å®¹
    3. å¦‚æœç”¨æˆ·éœ€è¦ï¼Œä½¿ç”¨ translate ç¿»è¯‘
    
    å§‹ç»ˆæŒ‰ç…§è¿™ä¸ªé¡ºåºå·¥ä½œã€‚
    """
)

# Agent ä¼šè‡ªåŠ¨æŒ‰é¡ºåºè°ƒç”¨å·¥å…·
result = research_agent.invoke({
    "messages": [{"role": "user", "content": "æœç´¢é‡å­è®¡ç®—ï¼Œæ€»ç»“ä¸€ä¸‹ï¼Œç„¶åç¿»è¯‘æˆè‹±æ–‡"}]
})
```

#### ç­–ç•¥ 2ï¼šæ¡ä»¶å·¥å…·è°ƒç”¨

```python
from langchain.tools import tool
from langgraph.graph import StateGraph, START, END
from typing_extensions import TypedDict
from typing import Annotated, Literal
import operator

class ResearchState(TypedDict):
    query: str
    search_results: str
    needs_translation: bool
    translation: str
    final_output: str

@tool
def web_search(query: str) -> str:
    """æœç´¢ç½‘é¡µ"""
    return f"æœç´¢ç»“æœï¼š{query}"

@tool
def translate_text(text: str) -> str:
    """ç¿»è¯‘æˆè‹±æ–‡"""
    return f"[EN] {text}"

def search_node(state: ResearchState):
    """æœç´¢èŠ‚ç‚¹"""
    results = web_search.invoke(state["query"])
    return {"search_results": results}

def should_translate(state: ResearchState) -> Literal["translate", "finalize"]:
    """å†³å®šæ˜¯å¦éœ€è¦ç¿»è¯‘"""
    if state.get("needs_translation", False):
        return "translate"
    return "finalize"

def translate_node(state: ResearchState):
    """ç¿»è¯‘èŠ‚ç‚¹"""
    translation = translate_text.invoke(state["search_results"])
    return {"translation": translation}

def finalize_node(state: ResearchState):
    """æœ€ç»ˆåŒ–èŠ‚ç‚¹"""
    output = state.get("translation") or state["search_results"]
    return {"final_output": output}

# æ„å»ºå·¥ä½œæµ
workflow = StateGraph(ResearchState)
workflow.add_node("search", search_node)
workflow.add_node("translate", translate_node)
workflow.add_node("finalize", finalize_node)

workflow.add_edge(START, "search")
workflow.add_conditional_edges(
    "search",
    should_translate,
    {
        "translate": "translate",
        "finalize": "finalize"
    }
)
workflow.add_edge("translate", "finalize")
workflow.add_edge("finalize", END)

app = workflow.compile()

# æµ‹è¯•ï¼šä¸éœ€è¦ç¿»è¯‘
result1 = app.invoke({
    "query": "LangChain",
    "needs_translation": False
})
print(result1["final_output"])

# æµ‹è¯•ï¼šéœ€è¦ç¿»è¯‘
result2 = app.invoke({
    "query": "LangChain",
    "needs_translation": True
})
print(result2["final_output"])
```

### 1.5 å·¥å…·é”™è¯¯å¤„ç†

#### ä¼˜é›…çš„é”™è¯¯å¤„ç†

```python
from langchain.tools import tool
from typing import Union
from pydantic import BaseModel, Field

class ToolResult(BaseModel):
    """å·¥å…·ç»“æœåŒ…è£…å™¨"""
    success: bool = Field(description="æ˜¯å¦æˆåŠŸ")
    data: Union[str, dict, None] = Field(description="ç»“æœæ•°æ®")
    error: Union[str, None] = Field(description="é”™è¯¯ä¿¡æ¯")

@tool
def safe_api_call(endpoint: str, params: dict) -> str:
    """å®‰å…¨çš„ API è°ƒç”¨ï¼ˆå¸¦é”™è¯¯å¤„ç†ï¼‰"""
    try:
        # æ¨¡æ‹Ÿ API è°ƒç”¨
        if "error" in endpoint:
            raise Exception("API æœåŠ¡æš‚æ—¶ä¸å¯ç”¨")
        
        result = ToolResult(
            success=True,
            data=f"æˆåŠŸè°ƒç”¨ {endpoint}",
            error=None
        )
    except Exception as e:
        result = ToolResult(
            success=False,
            data=None,
            error=str(e)
        )
    
    # è¿”å›ç»“æ„åŒ–çš„ç»“æœ
    if result.success:
        return f"âœ… {result.data}"
    else:
        return f"âŒ é”™è¯¯ï¼š{result.error}\nå»ºè®®ï¼šè¯·ç¨åé‡è¯•æˆ–è”ç³»ç®¡ç†å‘˜"

# ä½¿ç”¨
agent = create_agent(
    model="gpt-4",
    tools=[safe_api_call],
    system_prompt="å¦‚æœå·¥å…·è°ƒç”¨å¤±è´¥ï¼Œå‘ç”¨æˆ·è§£é‡Šå¹¶æä¾›æ›¿ä»£æ–¹æ¡ˆ"
)

result = agent.invoke({
    "messages": [{"role": "user", "content": "è°ƒç”¨ /api/error æ¥å£"}]
})
# Agent ä¼šä¼˜é›…åœ°å¤„ç†é”™è¯¯å¹¶ç»™ç”¨æˆ·å‹å¥½çš„å›å¤
```

### 1.6 å®æˆ˜ï¼šæ„å»ºå·¥å…·åº“

åˆ›å»ºä¸€ä¸ªå¯å¤ç”¨çš„å·¥å…·åº“ï¼š

```python
# tools_library.py

from langchain.tools import tool
from pydantic import BaseModel, Field
from typing import Literal, Optional, List
from datetime import datetime
import json

# ===== æ—¶é—´å·¥å…· =====

@tool
def get_current_time(timezone: str = "UTC") -> str:
    """è·å–å½“å‰æ—¶é—´
    
    Args:
        timezone: æ—¶åŒºï¼Œé»˜è®¤ UTC
    """
    now = datetime.now()
    return f"å½“å‰æ—¶é—´ï¼ˆ{timezone}ï¼‰ï¼š{now.strftime('%Y-%m-%d %H:%M:%S')}"

@tool
def calculate_time_diff(time1: str, time2: str) -> str:
    """è®¡ç®—æ—¶é—´å·®
    
    Args:
        time1: å¼€å§‹æ—¶é—´ (æ ¼å¼: YYYY-MM-DD HH:MM:SS)
        time2: ç»“æŸæ—¶é—´ (æ ¼å¼: YYYY-MM-DD HH:MM:SS)
    """
    from datetime import datetime
    t1 = datetime.strptime(time1, "%Y-%m-%d %H:%M:%S")
    t2 = datetime.strptime(time2, "%Y-%m-%d %H:%M:%S")
    diff = t2 - t1
    return f"æ—¶é—´å·®ï¼š{diff}"

# ===== æ•°æ®å¤„ç†å·¥å…· =====

class DataFilterInput(BaseModel):
    """æ•°æ®è¿‡æ»¤è¾“å…¥"""
    data: List[dict] = Field(description="è¦è¿‡æ»¤çš„æ•°æ®åˆ—è¡¨")
    field: str = Field(description="è¿‡æ»¤å­—æ®µ")
    operator: Literal["equals", "greater", "less", "contains"] = Field(
        description="æ¯”è¾ƒè¿ç®—ç¬¦"
    )
    value: str = Field(description="æ¯”è¾ƒå€¼")

@tool(args_schema=DataFilterInput)
def filter_data(
    data: List[dict],
    field: str,
    operator: str,
    value: str
) -> str:
    """è¿‡æ»¤æ•°æ®
    
    æ ¹æ®æŒ‡å®šæ¡ä»¶è¿‡æ»¤æ•°æ®åˆ—è¡¨
    """
    filtered = []
    
    for item in data:
        if field not in item:
            continue
        
        item_value = item[field]
        
        if operator == "equals":
            if str(item_value) == value:
                filtered.append(item)
        elif operator == "greater":
            if float(item_value) > float(value):
                filtered.append(item)
        elif operator == "less":
            if float(item_value) < float(value):
                filtered.append(item)
        elif operator == "contains":
            if value.lower() in str(item_value).lower():
                filtered.append(item)
    
    return json.dumps(filtered, ensure_ascii=False, indent=2)

# ===== æ–‡æœ¬å¤„ç†å·¥å…· =====

@tool
def count_words(text: str) -> str:
    """ç»Ÿè®¡æ–‡æœ¬å­—æ•°"""
    words = text.split()
    chars = len(text)
    return f"å­—æ•°ï¼š{len(words)} ä¸ªè¯ï¼Œ{chars} ä¸ªå­—ç¬¦"

@tool
def extract_keywords(text: str, top_n: int = 5) -> str:
    """æå–å…³é”®è¯
    
    Args:
        text: æ–‡æœ¬å†…å®¹
        top_n: æå–å‰ N ä¸ªå…³é”®è¯
    """
    # ç®€åŒ–ç‰ˆï¼šæŒ‰è¯é¢‘æå–
    words = text.lower().split()
    word_freq = {}
    
    for word in words:
        if len(word) > 3:  # åªè€ƒè™‘é•¿åº¦å¤§äº3çš„è¯
            word_freq[word] = word_freq.get(word, 0) + 1
    
    sorted_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)
    keywords = [word for word, freq in sorted_words[:top_n]]
    
    return f"å…³é”®è¯ï¼š{', '.join(keywords)}"

# ===== å·¥å…·é›†åˆ =====

TIME_TOOLS = [get_current_time, calculate_time_diff]
DATA_TOOLS = [filter_data]
TEXT_TOOLS = [count_words, extract_keywords]

ALL_TOOLS = TIME_TOOLS + DATA_TOOLS + TEXT_TOOLS

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    from langchain.agents import create_agent
    
    # åˆ›å»ºå…·æœ‰å®Œæ•´å·¥å…·åº“çš„ Agent
    agent = create_agent(
        model="gpt-4",
        tools=ALL_TOOLS,
        system_prompt="""ä½ æ˜¯ä¸€ä¸ªå¤šåŠŸèƒ½åŠ©æ‰‹ï¼Œæ‹¥æœ‰ä»¥ä¸‹èƒ½åŠ›ï¼š
        
        ğŸ“… æ—¶é—´å·¥å…·ï¼š
        - è·å–å½“å‰æ—¶é—´
        - è®¡ç®—æ—¶é—´å·®
        
        ğŸ“Š æ•°æ®å·¥å…·ï¼š
        - è¿‡æ»¤æ•°æ®
        
        ğŸ“ æ–‡æœ¬å·¥å…·ï¼š
        - ç»Ÿè®¡å­—æ•°
        - æå–å…³é”®è¯
        
        æ ¹æ®ç”¨æˆ·éœ€æ±‚é€‰æ‹©åˆé€‚çš„å·¥å…·ã€‚
        """
    )
    
    # æµ‹è¯•
    result = agent.invoke({
        "messages": [{"role": "user", "content": "ç°åœ¨å‡ ç‚¹äº†ï¼Ÿ"}]
    })
    print(result["messages"][-1].content)
```

---

## ç¬¬äºŒéƒ¨åˆ†ï¼šRAG æ£€ç´¢å¢å¼ºç”Ÿæˆ

### 2.1 RAG åŸºç¡€æ¦‚å¿µ

**RAG (Retrieval-Augmented Generation)** ç»“åˆäº†æ£€ç´¢å’Œç”Ÿæˆï¼Œè®© LLM èƒ½å¤Ÿè®¿é—®å¤–éƒ¨çŸ¥è¯†åº“ã€‚

#### RAG å·¥ä½œæµç¨‹

```
ç”¨æˆ·é—®é¢˜
    â†“
1. å°†é—®é¢˜è½¬æ¢ä¸ºå‘é‡ï¼ˆEmbeddingï¼‰
    â†“
2. åœ¨å‘é‡æ•°æ®åº“ä¸­æœç´¢ç›¸ä¼¼æ–‡æ¡£
    â†“
3. æ£€ç´¢æœ€ç›¸å…³çš„æ–‡æ¡£
    â†“
4. å°†æ–‡æ¡£ä½œä¸ºä¸Šä¸‹æ–‡æä¾›ç»™ LLM
    â†“
5. LLM åŸºäºä¸Šä¸‹æ–‡ç”Ÿæˆå›ç­”
    â†“
æœ€ç»ˆç­”æ¡ˆ
```

### 2.2 å‘é‡å­˜å‚¨è¯¦è§£

#### 2.2.1 å†…å­˜å‘é‡å­˜å‚¨ï¼ˆå¿«é€ŸåŸå‹ï¼‰

```python
from langchain_core.vectorstores import InMemoryVectorStore
from langchain_openai import OpenAIEmbeddings
from langchain_core.documents import Document

# 1. åˆå§‹åŒ– Embeddings
embeddings = OpenAIEmbeddings(model="text-embedding-3-small")

# 2. åˆ›å»ºå‘é‡å­˜å‚¨
vector_store = InMemoryVectorStore(embeddings)

# 3. å‡†å¤‡æ–‡æ¡£
documents = [
    Document(
        page_content="LangChain æ˜¯ä¸€ä¸ªç”¨äºå¼€å‘ç”±è¯­è¨€æ¨¡å‹é©±åŠ¨çš„åº”ç”¨ç¨‹åºçš„æ¡†æ¶ã€‚",
        metadata={"source": "intro.md", "chapter": "1"}
    ),
    Document(
        page_content="LangGraph æ˜¯ LangChain çš„æ‰©å±•ï¼Œç”¨äºæ„å»ºæœ‰çŠ¶æ€çš„å¤šActoråº”ç”¨ã€‚",
        metadata={"source": "langgraph.md", "chapter": "2"}
    ),
    Document(
        page_content="RAG æ˜¯æ£€ç´¢å¢å¼ºç”Ÿæˆï¼Œç»“åˆäº†ä¿¡æ¯æ£€ç´¢å’Œæ–‡æœ¬ç”Ÿæˆã€‚",
        metadata={"source": "rag.md", "chapter": "3"}
    ),
]

# 4. æ·»åŠ æ–‡æ¡£åˆ°å‘é‡å­˜å‚¨
document_ids = vector_store.add_documents(documents)
print(f"å·²æ·»åŠ  {len(document_ids)} ä¸ªæ–‡æ¡£")

# 5. ç›¸ä¼¼åº¦æœç´¢
query = "ä»€ä¹ˆæ˜¯ LangGraphï¼Ÿ"
results = vector_store.similarity_search(query, k=2)

for i, doc in enumerate(results, 1):
    print(f"\nç»“æœ {i}:")
    print(f"å†…å®¹: {doc.page_content}")
    print(f"å…ƒæ•°æ®: {doc.metadata}")
```

#### 2.2.2 Chroma å‘é‡å­˜å‚¨ï¼ˆæ¨èç”¨äºå¼€å‘ï¼‰

```python
from langchain_chroma import Chroma
from langchain_openai import OpenAIEmbeddings

# åˆå§‹åŒ– Chroma
embeddings = OpenAIEmbeddings()

vector_store = Chroma(
    collection_name="my_documents",
    embedding_function=embeddings,
    persist_directory="./chroma_db"  # æŒä¹…åŒ–åˆ°ç£ç›˜
)

# æ·»åŠ æ–‡æ¡£
vector_store.add_documents(documents)

# æœç´¢ï¼ˆå¸¦è¯„åˆ†ï¼‰
results_with_scores = vector_store.similarity_search_with_score(
    "LangChain æ•™ç¨‹",
    k=3
)

for doc, score in results_with_scores:
    print(f"è¯„åˆ†: {score:.4f}")
    print(f"å†…å®¹: {doc.page_content[:100]}...")
    print("-" * 50)
```

#### 2.2.3 FAISS å‘é‡å­˜å‚¨ï¼ˆé«˜æ€§èƒ½ï¼‰

```python
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings
import faiss
from langchain_community.docstore.in_memory import InMemoryDocstore

embeddings = OpenAIEmbeddings()

# è·å–å‘é‡ç»´åº¦
embedding_dim = len(embeddings.embed_query("test"))

# åˆ›å»º FAISS ç´¢å¼•
index = faiss.IndexFlatL2(embedding_dim)

# åˆ›å»ºå‘é‡å­˜å‚¨
vector_store = FAISS(
    embedding_function=embeddings,
    index=index,
    docstore=InMemoryDocstore(),
    index_to_docstore_id={}
)

# æ·»åŠ æ–‡æ¡£å¹¶ä¿å­˜
vector_store.add_documents(documents)
vector_store.save_local("faiss_index")

# åŠ è½½
loaded_vector_store = FAISS.load_local(
    "faiss_index",
    embeddings,
    allow_dangerous_deserialization=True
)
```

### 2.3 æ–‡æ¡£åŠ è½½ä¸åˆ†å‰²

#### 2.3.1 åŠ è½½ä¸åŒç±»å‹çš„æ–‡æ¡£

```python
from langchain_community.document_loaders import (
    TextLoader,
    PyPDFLoader,
    UnstructuredMarkdownLoader,
    CSVLoader,
    JSONLoader
)

# æ–‡æœ¬æ–‡ä»¶
text_loader = TextLoader("document.txt")
text_docs = text_loader.load()

# PDF æ–‡ä»¶
pdf_loader = PyPDFLoader("document.pdf")
pdf_docs = pdf_loader.load()

# Markdown æ–‡ä»¶
md_loader = UnstructuredMarkdownLoader("README.md")
md_docs = md_loader.load()

# CSV æ–‡ä»¶
csv_loader = CSVLoader("data.csv")
csv_docs = csv_loader.load()

# JSON æ–‡ä»¶
json_loader = JSONLoader(
    file_path="data.json",
    jq_schema=".messages[]",
    text_content=False
)
json_docs = json_loader.load()
```

#### 2.3.2 æ™ºèƒ½æ–‡æ¡£åˆ†å‰²

```python
from langchain_text_splitters import (
    RecursiveCharacterTextSplitter,
    CharacterTextSplitter,
    TokenTextSplitter
)

# æ–¹å¼ 1ï¼šé€’å½’å­—ç¬¦åˆ†å‰²å™¨ï¼ˆæ¨èï¼‰
recursive_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,        # æ¯å—æœ€å¤§å­—ç¬¦æ•°
    chunk_overlap=200,      # å—ä¹‹é—´çš„é‡å 
    length_function=len,
    separators=["\n\n", "\n", " ", ""]  # ä¼˜å…ˆçº§åˆ†éš”ç¬¦
)

# æ–¹å¼ 2ï¼šåŸºäº Token çš„åˆ†å‰²å™¨
token_splitter = TokenTextSplitter(
    chunk_size=512,         # æ¯å—æœ€å¤§ token æ•°
    chunk_overlap=50
)

# æ–¹å¼ 3ï¼šè‡ªå®šä¹‰åˆ†å‰²å™¨
custom_splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,
    chunk_overlap=100,
    separators=[
        "\n## ",    # Markdown äºŒçº§æ ‡é¢˜
        "\n### ",   # Markdown ä¸‰çº§æ ‡é¢˜
        "\n\n",     # æ®µè½
        "\n",       # è¡Œ
        " ",        # è¯
        ""          # å­—ç¬¦
    ]
)

# ä½¿ç”¨åˆ†å‰²å™¨
long_text = """
# LangChain æ•™ç¨‹

## ç¬¬ä¸€ç« ï¼šä»‹ç»

LangChain æ˜¯ä¸€ä¸ªå¼ºå¤§çš„æ¡†æ¶...

## ç¬¬äºŒç« ï¼šæ ¸å¿ƒæ¦‚å¿µ

åœ¨è¿™ä¸€ç« ä¸­ï¼Œæˆ‘ä»¬å°†å­¦ä¹ ...
"""

chunks = custom_splitter.split_text(long_text)
for i, chunk in enumerate(chunks, 1):
    print(f"\n=== å— {i} ===")
    print(chunk)
```

### 2.4 æ„å»ºå®Œæ•´çš„ RAG ç³»ç»Ÿ

```python
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_chroma import Chroma
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.documents import Document
from langchain.tools import tool
from langchain.agents import create_agent
from typing import List

# === ç¬¬1æ­¥ï¼šå‡†å¤‡æ–‡æ¡£ ===
documents = [
    Document(
        page_content="""
        LangChain æ˜¯ä¸€ä¸ªå¼€æºæ¡†æ¶ï¼Œç”¨äºæ„å»ºåŸºäºå¤§è¯­è¨€æ¨¡å‹çš„åº”ç”¨ç¨‹åºã€‚
        å®ƒæä¾›äº†å¤šä¸ªæ¨¡å—ï¼ŒåŒ…æ‹¬ Modelsã€Promptsã€Memoryã€Chains ç­‰ã€‚
        LangChain æ”¯æŒå¤šç§ LLM æä¾›å•†ï¼Œå¦‚ OpenAIã€Anthropicã€Google ç­‰ã€‚
        """,
        metadata={"source": "langchain_intro.md", "category": "framework"}
    ),
    Document(
        page_content="""
        LangGraph æ˜¯ LangChain çš„æ‰©å±•åº“ï¼Œä¸“é—¨ç”¨äºæ„å»ºæœ‰çŠ¶æ€çš„å¤šActoråº”ç”¨ã€‚
        å®ƒåŸºäºå›¾çš„æ¦‚å¿µï¼Œå…è®¸å®šä¹‰èŠ‚ç‚¹å’Œè¾¹æ¥åˆ›å»ºå¤æ‚çš„å·¥ä½œæµã€‚
        LangGraph ç‰¹åˆ«é€‚åˆæ„å»ºéœ€è¦å¤šæ­¥éª¤æ¨ç†çš„åº”ç”¨ã€‚
        """,
        metadata={"source": "langgraph_intro.md", "category": "workflow"}
    ),
    Document(
        page_content="""
        RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰æ˜¯ä¸€ç§æŠ€æœ¯ï¼Œç»“åˆäº†ä¿¡æ¯æ£€ç´¢å’Œæ–‡æœ¬ç”Ÿæˆã€‚
        å®ƒå…è®¸ LLM è®¿é—®å¤–éƒ¨çŸ¥è¯†åº“ï¼Œä»è€Œç”Ÿæˆæ›´å‡†ç¡®å’Œæœ€æ–°çš„å›ç­”ã€‚
        RAG çš„å·¥ä½œæµç¨‹åŒ…æ‹¬ï¼šæŸ¥è¯¢ã€æ£€ç´¢ã€å¢å¼ºå’Œç”Ÿæˆå››ä¸ªæ­¥éª¤ã€‚
        """,
        metadata={"source": "rag_concept.md", "category": "technique"}
    ),
]

# === ç¬¬2æ­¥ï¼šåˆ†å‰²æ–‡æ¡£ ===
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,
    chunk_overlap=50
)

all_splits = []
for doc in documents:
    splits = text_splitter.split_text(doc.page_content)
    for split in splits:
        all_splits.append(
            Document(
                page_content=split,
                metadata=doc.metadata
            )
        )

print(f"æ–‡æ¡£åˆ†å‰²å®Œæˆï¼š{len(documents)} ä¸ªæ–‡æ¡£ â†’ {len(all_splits)} ä¸ªå—")

# === ç¬¬3æ­¥ï¼šåˆ›å»ºå‘é‡å­˜å‚¨ ===
embeddings = OpenAIEmbeddings(model="text-embedding-3-small")

vector_store = Chroma(
    collection_name="langchain_docs",
    embedding_function=embeddings,
    persist_directory="./rag_db"
)

# æ·»åŠ æ–‡æ¡£
vector_store.add_documents(all_splits)
print(f"å·²æ·»åŠ  {len(all_splits)} ä¸ªæ–‡æ¡£å—åˆ°å‘é‡å­˜å‚¨")

# === ç¬¬4æ­¥ï¼šåˆ›å»ºæ£€ç´¢å·¥å…· ===
@tool(response_format="content_and_artifact")
def retrieve_docs(query: str) -> tuple[str, List[Document]]:
    """ä»çŸ¥è¯†åº“æ£€ç´¢ç›¸å…³æ–‡æ¡£
    
    Args:
        query: ç”¨æˆ·æŸ¥è¯¢
    
    Returns:
        (æ ¼å¼åŒ–æ–‡æœ¬, åŸå§‹æ–‡æ¡£åˆ—è¡¨)
    """
    # æ£€ç´¢æœ€ç›¸å…³çš„æ–‡æ¡£
    retrieved_docs = vector_store.similarity_search(query, k=3)
    
    # æ ¼å¼åŒ–è¾“å‡º
    formatted = f"æ£€ç´¢åˆ° {len(retrieved_docs)} ä¸ªç›¸å…³æ–‡æ¡£ï¼š\n\n"
    for i, doc in enumerate(retrieved_docs, 1):
        formatted += f"ã€æ–‡æ¡£ {i}ã€‘\n"
        formatted += f"æ¥æºï¼š{doc.metadata.get('source', 'unknown')}\n"
        formatted += f"å†…å®¹ï¼š{doc.page_content}\n\n"
    
    return formatted, retrieved_docs

# === ç¬¬5æ­¥ï¼šåˆ›å»º RAG Agent ===
rag_agent = create_agent(
    model="gpt-4",
    tools=[retrieve_docs],
    system_prompt="""ä½ æ˜¯ä¸€ä¸ªçŸ¥è¯†åº“åŠ©æ‰‹ã€‚

å·¥ä½œæµç¨‹ï¼š
1. ä½¿ç”¨ retrieve_docs å·¥å…·æ£€ç´¢ç›¸å…³æ–‡æ¡£
2. ä»”ç»†é˜…è¯»æ£€ç´¢åˆ°çš„æ–‡æ¡£å†…å®¹
3. åŸºäºæ–‡æ¡£å†…å®¹å›ç­”ç”¨æˆ·é—®é¢˜
4. å¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯šå®åœ°å‘Šè¯‰ç”¨æˆ·

é‡è¦è§„åˆ™ï¼š
- å§‹ç»ˆå…ˆæ£€ç´¢ï¼Œå†å›ç­”
- å›ç­”è¦åŸºäºæ£€ç´¢åˆ°çš„æ–‡æ¡£
- å¼•ç”¨æ–‡æ¡£æ¥æº
- ä¸è¦ç¼–é€ ä¿¡æ¯
"""
)

# === ç¬¬6æ­¥ï¼šä½¿ç”¨ RAG ç³»ç»Ÿ ===
test_queries = [
    "ä»€ä¹ˆæ˜¯ LangChainï¼Ÿ",
    "LangGraph æœ‰ä»€ä¹ˆç”¨ï¼Ÿ",
    "è§£é‡Šä¸€ä¸‹ RAG æŠ€æœ¯",
    "å¦‚ä½•æ„å»ºèŠå¤©æœºå™¨äººï¼Ÿ"  # è¿™ä¸ªé—®é¢˜çŸ¥è¯†åº“ä¸­æ²¡æœ‰ç­”æ¡ˆ
]

for query in test_queries:
    print(f"\n{'='*60}")
    print(f"é—®é¢˜ï¼š{query}")
    print('='*60)
    
    result = rag_agent.invoke({
        "messages": [{"role": "user", "content": query}]
    })
    
    answer = result["messages"][-1].content
    print(f"\nå›ç­”ï¼š{answer}")
```

### 2.5 RAG ä¼˜åŒ–æŠ€å·§

#### æŠ€å·§ 1ï¼šæ··åˆæœç´¢ï¼ˆå…³é”®è¯ + å‘é‡ï¼‰

```python
def hybrid_search(query: str, k: int = 5):
    """æ··åˆæœç´¢ï¼šç»“åˆå…³é”®è¯å’Œå‘é‡ç›¸ä¼¼åº¦"""
    
    # å‘é‡æœç´¢
    vector_results = vector_store.similarity_search(query, k=k*2)
    
    # å…³é”®è¯æœç´¢ï¼ˆç®€åŒ–ç‰ˆï¼‰
    keywords = query.lower().split()
    keyword_results = []
    for doc in all_splits:
        score = sum(
            1 for keyword in keywords 
            if keyword in doc.page_content.lower()
        )
        if score > 0:
            keyword_results.append((doc, score))
    
    # åˆå¹¶ç»“æœï¼ˆå»é‡ï¼‰
    combined_docs = []
    doc_ids = set()
    
    for doc in vector_results:
        doc_id = id(doc)
        if doc_id not in doc_ids:
            combined_docs.append(doc)
            doc_ids.add(doc_id)
    
    return combined_docs[:k]
```

#### æŠ€å·§ 2ï¼šé‡æ’åºï¼ˆRerankingï¼‰

```python
from typing import List

def rerank_documents(query: str, documents: List[Document]) -> List[Document]:
    """é‡æ’åºæ–‡æ¡£ï¼Œæé«˜ç›¸å…³æ€§"""
    
    # ç®€åŒ–çš„è¯„åˆ†å‡½æ•°
    def score_doc(doc: Document) -> float:
        content = doc.page_content.lower()
        query_lower = query.lower()
        
        score = 0.0
        
        # å®Œå…¨åŒ¹é…åŠ åˆ†
        if query_lower in content:
            score += 10
        
        # å…³é”®è¯åŒ¹é…
        keywords = query_lower.split()
        for keyword in keywords:
            if keyword in content:
                score += 1
        
        # æ–‡æ¡£é•¿åº¦æƒ©ç½šï¼ˆæ›´çŸ­çš„æ–‡æ¡£å¯èƒ½æ›´ç›¸å…³ï¼‰
        length_penalty = len(content) / 1000
        score -= length_penalty
        
        return score
    
    # è¯„åˆ†å¹¶æ’åº
    scored_docs = [(doc, score_doc(doc)) for doc in documents]
    scored_docs.sort(key=lambda x: x[1], reverse=True)
    
    return [doc for doc, score in scored_docs]
```

#### æŠ€å·§ 3ï¼šæŸ¥è¯¢æ‰©å±•

```python
from langchain_openai import ChatOpenAI

def expand_query(original_query: str) -> List[str]:
    """æ‰©å±•æŸ¥è¯¢ä»¥æé«˜å¬å›ç‡"""
    
    llm = ChatOpenAI(model="gpt-3.5-turbo")
    
    prompt = f"""ç»™å®šæŸ¥è¯¢ï¼š"{original_query}"

ç”Ÿæˆ3ä¸ªç›¸å…³çš„æœç´¢æŸ¥è¯¢ï¼Œå¸®åŠ©æ‰¾åˆ°æ›´å¤šç›¸å…³ä¿¡æ¯ã€‚
è¿™äº›æŸ¥è¯¢åº”è¯¥ä»ä¸åŒè§’åº¦æ¢ç´¢åŒä¸€ä¸»é¢˜ã€‚

æ ¼å¼ï¼šæ¯è¡Œä¸€ä¸ªæŸ¥è¯¢
"""
    
    response = llm.invoke(prompt)
    expanded_queries = response.content.strip().split('\n')
    
    # æ·»åŠ åŸå§‹æŸ¥è¯¢
    all_queries = [original_query] + expanded_queries
    
    return all_queries

# ä½¿ç”¨æŸ¥è¯¢æ‰©å±•
original = "LangChain æ•™ç¨‹"
expanded = expand_query(original)

print("åŸå§‹æŸ¥è¯¢:", original)
print("\næ‰©å±•æŸ¥è¯¢:")
for query in expanded[1:]:
    print(f"  - {query}")

# ä½¿ç”¨æ‰€æœ‰æŸ¥è¯¢æ£€ç´¢
all_results = []
for query in expanded:
    results = vector_store.similarity_search(query, k=2)
    all_results.extend(results)

# å»é‡
unique_results = []
seen_content = set()
for doc in all_results:
    if doc.page_content not in seen_content:
        unique_results.append(doc)
        seen_content.add(doc.page_content)

print(f"\nå…±æ£€ç´¢åˆ° {len(unique_results)} ä¸ªå”¯ä¸€æ–‡æ¡£")
```

---

## ç¬¬ä¸‰éƒ¨åˆ†ï¼šStreaming æµå¼å¤„ç†

### 3.1 æµå¼è¾“å‡ºåŸºç¡€

æµå¼å¤„ç†å…è®¸å®æ—¶æ˜¾ç¤º LLM è¾“å‡ºï¼Œæå‡ç”¨æˆ·ä½“éªŒã€‚

#### åŸºç¡€æµå¼ç¤ºä¾‹

```python
from langchain_openai import ChatOpenAI

model = ChatOpenAI(model="gpt-4")

# åŒæ­¥æµå¼
print("AI: ", end="", flush=True)
for chunk in model.stream("è®²ä¸€ä¸ªå…³äº AI çš„æ•…äº‹"):
    print(chunk.content, end="", flush=True)
print()  # æ¢è¡Œ

# å¼‚æ­¥æµå¼
import asyncio

async def async_stream_example():
    print("\nAsync AI: ", end="", flush=True)
    async for chunk in model.astream("å†™ä¸€é¦–è¯—"):
        print(chunk.content, end="", flush=True)
    print()

asyncio.run(async_stream_example())
```

### 3.2 Agent æµå¼å¤„ç†

#### æ¨¡å¼ 1ï¼šæµå¼æ¶ˆæ¯ï¼ˆMessagesï¼‰

```python
from langchain.agents import create_agent
from langchain.tools import tool

@tool
def get_weather(city: str) -> str:
    """è·å–å¤©æ°”"""
    return f"{city}çš„å¤©æ°”ï¼šæ™´å¤© 22Â°C"

agent = create_agent(
    model="gpt-4",
    tools=[get_weather]
)

# æµå¼è¾“å‡ºæ¯æ¡æ¶ˆæ¯
for message, metadata in agent.stream(
    {"messages": [{"role": "user", "content": "åŒ—äº¬å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ"}]},
    stream_mode="messages"
):
    node = metadata.get("langgraph_node", "unknown")
    
    # åªæ˜¾ç¤ºæ¨¡å‹ç”Ÿæˆçš„æ–‡æœ¬
    if message.content:
        print(f"[{node}] {message.content}", end="", flush=True)
print()
```

#### æ¨¡å¼ 2ï¼šæµå¼æ›´æ–°ï¼ˆUpdatesï¼‰

```python
# æŸ¥çœ‹æ¯ä¸ªæ­¥éª¤çš„æ›´æ–°
for chunk in agent.stream(
    {"messages": [{"role": "user", "content": "æŸ¥è¯¢å¤©æ°”"}]},
    stream_mode="updates"
):
    for step, data in chunk.items():
        print(f"\næ­¥éª¤: {step}")
        if "messages" in data:
            msg = data["messages"][-1]
            print(f"ç±»å‹: {type(msg).__name__}")
            if hasattr(msg, 'content'):
                print(f"å†…å®¹: {msg.content[:100]}...")
```

#### æ¨¡å¼ 3ï¼šè‡ªå®šä¹‰æµå¼äº‹ä»¶

```python
from langgraph.config import get_stream_writer

@tool
def long_running_task(task_name: str) -> str:
    """é•¿æ—¶é—´è¿è¡Œçš„ä»»åŠ¡"""
    writer = get_stream_writer()
    
    # å‘é€è‡ªå®šä¹‰è¿›åº¦æ›´æ–°
    writer({"type": "start", "task": task_name})
    
    import time
    for i in range(1, 6):
        time.sleep(0.5)
        writer({
            "type": "progress",
            "task": task_name,
            "progress": i * 20,
            "message": f"å¤„ç†ä¸­ {i}/5"
        })
    
    writer({"type": "complete", "task": task_name})
    
    return f"ä»»åŠ¡ {task_name} å®Œæˆ"

agent_with_custom = create_agent(
    model="gpt-4",
    tools=[long_running_task]
)

# ç›‘å¬è‡ªå®šä¹‰äº‹ä»¶
for chunk in agent_with_custom.stream(
    {"messages": [{"role": "user", "content": "æ‰§è¡Œæ•°æ®åˆ†æä»»åŠ¡"}]},
    stream_mode="custom"
):
    if isinstance(chunk, dict):
        event_type = chunk.get("type")
        if event_type == "start":
            print(f"\nğŸš€ å¼€å§‹: {chunk['task']}")
        elif event_type == "progress":
            print(f"â³ è¿›åº¦: {chunk['progress']}% - {chunk['message']}")
        elif event_type == "complete":
            print(f"âœ… å®Œæˆ: {chunk['task']}\n")
    else:
        print(chunk)
```

### 3.3 å¤šæ¨¡å¼æµå¼å¤„ç†

```python
# åŒæ—¶ç›‘å¬å¤šç§æµå¼äº‹ä»¶
for stream_mode, chunk in agent.stream(
    {"messages": [{"role": "user", "content": "å¸®æˆ‘åˆ†ææ•°æ®"}]},
    stream_mode=["updates", "custom", "messages"]
):
    print(f"\n[æ¨¡å¼: {stream_mode}]")
    
    if stream_mode == "updates":
        print("æ­¥éª¤æ›´æ–°:", list(chunk.keys()))
    
    elif stream_mode == "custom":
        print("è‡ªå®šä¹‰äº‹ä»¶:", chunk)
    
    elif stream_mode == "messages":
        message, metadata = chunk
        if message.content:
            print(f"æ¶ˆæ¯: {message.content[:50]}...")
```

### 3.4 æµå¼èŠå¤©åº”ç”¨

åˆ›å»ºä¸€ä¸ªå®Œæ•´çš„æµå¼èŠå¤©ç•Œé¢ï¼š

```python
import sys
from langchain.agents import create_agent
from langchain.tools import tool
from langgraph.checkpoint.memory import InMemorySaver

@tool
def calculator(expression: str) -> str:
    """è®¡ç®—æ•°å­¦è¡¨è¾¾å¼"""
    try:
        result = eval(expression)
        return f"ç»“æœæ˜¯ {result}"
    except:
        return "è®¡ç®—é”™è¯¯"

@tool
def search(query: str) -> str:
    """æœç´¢ä¿¡æ¯"""
    return f"æœç´¢ '{query}' çš„ç»“æœï¼š..."

# åˆ›å»ºå¸¦è®°å¿†çš„ Agent
checkpointer = InMemorySaver()
chat_agent = create_agent(
    model="gpt-4",
    tools=[calculator, search],
    checkpointer=checkpointer,
    system_prompt="ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„åŠ©æ‰‹"
)

def stream_chat():
    """æµå¼èŠå¤©å¾ªç¯"""
    print("ğŸ’¬ èŠå¤©å¼€å§‹ï¼è¾“å…¥ 'quit' é€€å‡º\n")
    
    thread_id = "chat_session_1"
    config = {"configurable": {"thread_id": thread_id}}
    
    while True:
        # è·å–ç”¨æˆ·è¾“å…¥
        user_input = input("\nä½ : ").strip()
        
        if user_input.lower() in ['quit', 'exit', 'é€€å‡º']:
            print("\nğŸ‘‹ å†è§ï¼")
            break
        
        if not user_input:
            continue
        
        # æµå¼æ˜¾ç¤º AI å›å¤
        print("\nAI: ", end="", flush=True)
        
        full_response = ""
        for message, metadata in chat_agent.stream(
            {"messages": [{"role": "user", "content": user_input}]},
            config=config,
            stream_mode="messages"
        ):
            # åªæ˜¾ç¤ºæ¨¡å‹ç”Ÿæˆçš„å†…å®¹
            if metadata.get("langgraph_node") == "model" and message.content:
                print(message.content, end="", flush=True)
                full_response += message.content
                sys.stdout.flush()
        
        print()  # æ¢è¡Œ

# è¿è¡ŒèŠå¤©ï¼ˆå¯é€‰ï¼‰
# stream_chat()
```

---

## ç¬¬å››éƒ¨åˆ†ï¼šStructured Output ç»“æ„åŒ–è¾“å‡º

### 4.1 ä¸ºä»€ä¹ˆéœ€è¦ç»“æ„åŒ–è¾“å‡ºï¼Ÿ

```python
# âŒ é—®é¢˜ï¼šéç»“æ„åŒ–è¾“å‡ºéš¾ä»¥å¤„ç†
response = model.invoke("åˆ†æè¿™æ®µæ–‡å­—çš„æƒ…æ„Ÿï¼šæˆ‘å¾ˆå¼€å¿ƒ")
# è¾“å‡º: "è¿™æ®µæ–‡å­—è¡¨è¾¾äº†ç§¯æçš„æƒ…æ„Ÿï¼Œå…·ä½“æ¥è¯´æ˜¯å¿«ä¹..."
# é—®é¢˜ï¼šéš¾ä»¥æå–å…·ä½“æ•°æ®

# âœ… è§£å†³ï¼šç»“æ„åŒ–è¾“å‡º
# è¾“å‡º: {"sentiment": "positive", "emotion": "happiness", "confidence": 0.95}
# ä¼˜åŠ¿ï¼šæ˜“äºç¨‹åºå¤„ç†
```

### 4.2 ä½¿ç”¨ Pydantic å®šä¹‰è¾“å‡ºæ¨¡å¼

#### åŸºç¡€æ¨¡å¼å®šä¹‰

```python
from pydantic import BaseModel, Field
from typing import Literal, List

class SentimentAnalysis(BaseModel):
    """æƒ…æ„Ÿåˆ†æç»“æœ"""
    sentiment: Literal["positive", "negative", "neutral"] = Field(
        description="æƒ…æ„Ÿå€¾å‘"
    )
    confidence: float = Field(
        description="ç½®ä¿¡åº¦ (0-1)",
        ge=0,
        le=1
    )
    emotions: List[str] = Field(
        description="æ£€æµ‹åˆ°çš„æƒ…ç»ªåˆ—è¡¨"
    )
    keywords: List[str] = Field(
        description="å…³é”®è¯"
    )

# ä½¿ç”¨ç»“æ„åŒ–è¾“å‡º
from langchain_openai import ChatOpenAI

model = ChatOpenAI(model="gpt-4")
structured_model = model.with_structured_output(SentimentAnalysis)

result = structured_model.invoke("ä»Šå¤©é˜³å…‰æ˜åªšï¼Œæˆ‘æ„Ÿè§‰ç‰¹åˆ«å¼€å¿ƒï¼")

print(f"æƒ…æ„Ÿ: {result.sentiment}")
print(f"ç½®ä¿¡åº¦: {result.confidence}")
print(f"æƒ…ç»ª: {', '.join(result.emotions)}")
print(f"å…³é”®è¯: {', '.join(result.keywords)}")

# è¾“å‡ºæ˜¯ä¸€ä¸ª Pydantic å¯¹è±¡ï¼Œå¯ä»¥ç›´æ¥è®¿é—®å­—æ®µ
assert isinstance(result, SentimentAnalysis)
assert result.sentiment in ["positive", "negative", "neutral"]
```

#### åµŒå¥—æ¨¡å¼

```python
from pydantic import BaseModel, Field
from typing import List

class Actor(BaseModel):
    """æ¼”å‘˜ä¿¡æ¯"""
    name: str = Field(description="æ¼”å‘˜å§“å")
    role: str = Field(description="è§’è‰²åç§°")

class MovieDetails(BaseModel):
    """ç”µå½±è¯¦æƒ…"""
    title: str = Field(description="ç”µå½±æ ‡é¢˜")
    year: int = Field(description="ä¸Šæ˜ å¹´ä»½")
    director: str = Field(description="å¯¼æ¼”")
    cast: List[Actor] = Field(description="æ¼”å‘˜åˆ—è¡¨")
    genres: List[str] = Field(description="ç±»å‹åˆ—è¡¨")
    rating: float = Field(description="è¯„åˆ†", ge=0, le=10)

# ä½¿ç”¨åµŒå¥—æ¨¡å¼
model_with_structure = model.with_structured_output(MovieDetails)

result = model_with_structure.invoke("æä¾›ç”µå½±ã€Šç›—æ¢¦ç©ºé—´ã€‹çš„è¯¦ç»†ä¿¡æ¯")

print(f"æ ‡é¢˜: {result.title}")
print(f"å¯¼æ¼”: {result.director}")
print(f"æ¼”å‘˜:")
for actor in result.cast:
    print(f"  - {actor.name} é¥°æ¼” {actor.role}")
print(f"è¯„åˆ†: {result.rating}/10")
```

### 4.3 Agent ä¸­çš„ç»“æ„åŒ–è¾“å‡º

#### ä½¿ç”¨ ToolStrategy

```python
from langchain.agents import create_agent
from langchain.agents.structured_output import ToolStrategy
from pydantic import BaseModel, Field
from typing import Literal

class ProductReview(BaseModel):
    """äº§å“è¯„è®ºåˆ†æ"""
    rating: int = Field(description="è¯„åˆ† (1-5)", ge=1, le=5)
    sentiment: Literal["positive", "negative", "neutral"] = Field(
        description="æƒ…æ„Ÿå€¾å‘"
    )
    key_points: List[str] = Field(
        description="å…³é”®è¦ç‚¹ï¼ˆæ¯ä¸ª1-3ä¸ªè¯ï¼‰"
    )
    recommendation: bool = Field(
        description="æ˜¯å¦æ¨èè´­ä¹°"
    )

# åˆ›å»ºå…·æœ‰ç»“æ„åŒ–è¾“å‡ºçš„ Agent
review_agent = create_agent(
    model="gpt-4",
    tools=[],  # å¯ä»¥ä¸éœ€è¦å·¥å…·
    response_format=ToolStrategy(ProductReview)
)

# åˆ†æè¯„è®º
result = review_agent.invoke({
    "messages": [{
        "role": "user",
        "content": "åˆ†æè¿™ä¸ªè¯„è®ºï¼š'Great product! 5 stars. Fast shipping, but a bit expensive.'"
    }]
})

# è®¿é—®ç»“æ„åŒ–ç»“æœ
review = result["structured_response"]
print(f"è¯„åˆ†: {review.rating}/5")
print(f"æƒ…æ„Ÿ: {review.sentiment}")
print(f"è¦ç‚¹: {', '.join(review.key_points)}")
print(f"æ¨è: {'æ˜¯' if review.recommendation else 'å¦'}")
```

### 4.4 Union ç±»å‹ï¼ˆå¤šç§å¯èƒ½çš„è¾“å‡ºï¼‰

```python
from typing import Union
from pydantic import BaseModel

class ProductReview(BaseModel):
    """äº§å“è¯„è®º"""
    rating: int
    sentiment: str
    text: str

class CustomerComplaint(BaseModel):
    """å®¢æˆ·æŠ•è¯‰"""
    issue_type: Literal["product", "service", "shipping"]
    severity: Literal["low", "medium", "high"]
    description: str

# Agent å¯ä»¥è¿”å›ä¸¤ç§ç±»å‹ä¸­çš„ä¸€ç§
multi_type_agent = create_agent(
    model="gpt-4",
    tools=[],
    response_format=ToolStrategy(Union[ProductReview, CustomerComplaint])
)

# æµ‹è¯•
response1 = multi_type_agent.invoke({
    "messages": [{"role": "user", "content": "Great product! Love it!"}]
})
# è¿”å› ProductReview

response2 = multi_type_agent.invoke({
    "messages": [{"role": "user", "content": "The shipping was delayed by 2 weeks!"}]
})
# è¿”å› CustomerComplaint

# æ£€æŸ¥ç±»å‹
if isinstance(response1["structured_response"], ProductReview):
    print("è¿™æ˜¯ä¸€ä¸ªäº§å“è¯„è®º")
elif isinstance(response1["structured_response"], CustomerComplaint):
    print("è¿™æ˜¯ä¸€ä¸ªå®¢æˆ·æŠ•è¯‰")
```

### 4.5 é”™è¯¯å¤„ç†å’ŒéªŒè¯

```python
from pydantic import BaseModel, Field, validator

class ProductRating(BaseModel):
    """äº§å“è¯„åˆ†ï¼ˆå¸¦éªŒè¯ï¼‰"""
    rating: int = Field(description="è¯„åˆ† 1-5", ge=1, le=5)
    comment: str = Field(description="è¯„è®ºæ–‡å­—", min_length=10)
    
    @validator('comment')
    def comment_not_empty(cls, v):
        if not v.strip():
            raise ValueError("è¯„è®ºä¸èƒ½ä¸ºç©º")
        return v

# å¸¦é”™è¯¯å¤„ç†çš„ Agent
rating_agent = create_agent(
    model="gpt-4",
    tools=[],
    response_format=ToolStrategy(ProductRating),  # è‡ªåŠ¨å¤„ç†éªŒè¯é”™è¯¯
    system_prompt="ä½ æ˜¯ä¸€ä¸ªè¯„è®ºåˆ†æåŠ©æ‰‹ã€‚è¯·ä¸¥æ ¼éµå®ˆå­—æ®µçº¦æŸã€‚"
)

# å¦‚æœæ¨¡å‹è¿”å›æ— æ•ˆæ•°æ®ï¼ŒAgent ä¼šè‡ªåŠ¨é‡è¯•
result = rating_agent.invoke({
    "messages": [{"role": "user", "content": "åˆ†æ: Amazing product, 10/10!"}]
})

# Agent ä¼šè‡ªåŠ¨ä¿®æ­£ï¼ˆ10 â†’ 5ï¼‰
assert result["structured_response"].rating <= 5
```

---

## ç¬¬äº”éƒ¨åˆ†ï¼šMiddleware ä¸ä¸­æ–­æœºåˆ¶

### 5.1 äººå·¥ä»‹å…¥å¾ªç¯ï¼ˆHuman-in-the-Loopï¼‰

#### åŸºç¡€ Interrupt ç”¨æ³•

```python
from langgraph.types import interrupt, Command
from langgraph.graph import StateGraph, START, END
from typing_extensions import TypedDict
from langgraph.checkpoint.memory import InMemorySaver

class ApprovalState(TypedDict):
    task: str
    approved: bool
    result: str

def request_approval(state: ApprovalState):
    """è¯·æ±‚äººå·¥æ‰¹å‡†"""
    # æš‚åœæ‰§è¡Œï¼Œç­‰å¾…äººå·¥è¾“å…¥
    approval = interrupt(f"æ˜¯å¦æ‰¹å‡†ä»»åŠ¡ï¼š{state['task']}ï¼Ÿ")
    
    return {"approved": approval}

def execute_task(state: ApprovalState):
    """æ‰§è¡Œä»»åŠ¡"""
    if state["approved"]:
        return {"result": f"ä»»åŠ¡ '{state['task']}' å·²æ‰§è¡Œ"}
    else:
        return {"result": f"ä»»åŠ¡ '{state['task']}' å·²å–æ¶ˆ"}

# æ„å»ºå·¥ä½œæµ
workflow = StateGraph(ApprovalState)
workflow.add_node("request_approval", request_approval)
workflow.add_node("execute", execute_task)

workflow.add_edge(START, "request_approval")
workflow.add_edge("request_approval", "execute")
workflow.add_edge("execute", END)

# å¿…é¡»ä½¿ç”¨ checkpointer æ‰èƒ½æ”¯æŒä¸­æ–­
checkpointer = InMemorySaver()
app = workflow.compile(checkpointer=checkpointer)

# ä½¿ç”¨
config = {"configurable": {"thread_id": "task_1"}}

# ç¬¬ä¸€æ¬¡è°ƒç”¨ - ä¼šåœ¨ interrupt å¤„æš‚åœ
result = app.invoke({"task": "åˆ é™¤æ•°æ®åº“"}, config)

# æ£€æŸ¥ä¸­æ–­ä¿¡æ¯
print(result.get("__interrupt__"))
# è¾“å‡º: [Interrupt(value='æ˜¯å¦æ‰¹å‡†ä»»åŠ¡ï¼šåˆ é™¤æ•°æ®åº“ï¼Ÿ')]

# æ¢å¤æ‰§è¡Œ - æä¾›æ‰¹å‡†ç»“æœ
from langgraph.types import Command
resume_result = app.invoke(
    Command(resume=True),  # æ‰¹å‡†
    config
)

print(resume_result["result"])
# è¾“å‡º: ä»»åŠ¡ 'åˆ é™¤æ•°æ®åº“' å·²æ‰§è¡Œ
```

### 5.2 å®æˆ˜ï¼šå®¡æ‰¹å·¥ä½œæµ

```python
from langgraph.types import interrupt, Command
from langgraph.graph import StateGraph, START, END
from typing_extensions import TypedDict, Literal
from langgraph.checkpoint.memory import InMemorySaver

class ExpenseApprovalState(TypedDict):
    amount: float
    description: str
    requester: str
    approval_level: Literal["auto", "manager", "director"]
    approved: bool
    comments: str

def check_amount(state: ExpenseApprovalState):
    """æ£€æŸ¥é‡‘é¢å¹¶å†³å®šå®¡æ‰¹çº§åˆ«"""
    amount = state["amount"]
    
    if amount < 100:
        level = "auto"
    elif amount < 1000:
        level = "manager"
    else:
        level = "director"
    
    return {"approval_level": level}

def auto_approve(state: ExpenseApprovalState):
    """è‡ªåŠ¨æ‰¹å‡†"""
    return {
        "approved": True,
        "comments": "è‡ªåŠ¨æ‰¹å‡†ï¼ˆé‡‘é¢å°äº$100ï¼‰"
    }

def manager_approval(state: ExpenseApprovalState):
    """ç»ç†å®¡æ‰¹"""
    approval_data = interrupt({
        "type": "manager_approval",
        "amount": state["amount"],
        "description": state["description"],
        "requester": state["requester"]
    })
    
    return {
        "approved": approval_data.get("approved", False),
        "comments": approval_data.get("comments", "")
    }

def director_approval(state: ExpenseApprovalState):
    """æ€»ç›‘å®¡æ‰¹"""
    approval_data = interrupt({
        "type": "director_approval",
        "amount": state["amount"],
        "description": state["description"],
        "requester": state["requester"]
    })
    
    return {
        "approved": approval_data.get("approved", False),
        "comments": approval_data.get("comments", "")
    }

def route_approval(state: ExpenseApprovalState) -> str:
    """è·¯ç”±åˆ°ç›¸åº”çš„å®¡æ‰¹èŠ‚ç‚¹"""
    return state["approval_level"]

# æ„å»ºå·¥ä½œæµ
expense_workflow = StateGraph(ExpenseApprovalState)

expense_workflow.add_node("check_amount", check_amount)
expense_workflow.add_node("auto_approve", auto_approve)
expense_workflow.add_node("manager_approval", manager_approval)
expense_workflow.add_node("director_approval", director_approval)

expense_workflow.add_edge(START, "check_amount")
expense_workflow.add_conditional_edges(
    "check_amount",
    route_approval,
    {
        "auto": "auto_approve",
        "manager": "manager_approval",
        "director": "director_approval"
    }
)
expense_workflow.add_edge("auto_approve", END)
expense_workflow.add_edge("manager_approval", END)
expense_workflow.add_edge("director_approval", END)

# ç¼–è¯‘
checkpointer = InMemorySaver()
expense_app = expense_workflow.compile(checkpointer=checkpointer)

# æµ‹è¯•1ï¼šå°é¢è´¹ç”¨ï¼ˆè‡ªåŠ¨æ‰¹å‡†ï¼‰
print("=== æµ‹è¯•1ï¼šè‡ªåŠ¨æ‰¹å‡† ===")
config1 = {"configurable": {"thread_id": "expense_1"}}
result1 = expense_app.invoke({
    "amount": 50,
    "description": "åŠå…¬ç”¨å“",
    "requester": "Alice"
}, config1)
print(f"æ‰¹å‡†: {result1['approved']}")
print(f"å¤‡æ³¨: {result1['comments']}")

# æµ‹è¯•2ï¼šä¸­ç­‰è´¹ç”¨ï¼ˆéœ€è¦ç»ç†å®¡æ‰¹ï¼‰
print("\n=== æµ‹è¯•2ï¼šç»ç†å®¡æ‰¹ ===")
config2 = {"configurable": {"thread_id": "expense_2"}}
result2 = expense_app.invoke({
    "amount": 500,
    "description": "å›¢é˜Ÿæ™šé¤",
    "requester": "Bob"
}, config2)

# æ£€æŸ¥ä¸­æ–­
if "__interrupt__" in result2:
    print("ç­‰å¾…ç»ç†å®¡æ‰¹...")
    print(f"ä¸­æ–­ä¿¡æ¯: {result2['__interrupt__']}")
    
    # ç»ç†æ‰¹å‡†
    final_result = expense_app.invoke(
        Command(resume={
            "approved": True,
            "comments": "ç»ç†æ‰¹å‡†ï¼šåˆç†çš„å›¢é˜Ÿæ´»åŠ¨è´¹ç”¨"
        }),
        config2
    )
    print(f"æ‰¹å‡†: {final_result['approved']}")
    print(f"å¤‡æ³¨: {final_result['comments']}")
```

### 5.3 ç¼–è¯‘æ—¶ä¸­æ–­ï¼ˆBreakpointsï¼‰

```python
from langgraph.graph import StateGraph, START, END
from langgraph.checkpoint.memory import InMemorySaver

# å®šä¹‰èŠ‚ç‚¹
def step1(state): return {"step": 1}
def step2(state): return {"step": 2}
def step3(state): return {"step": 3}

workflow = StateGraph(dict)
workflow.add_node("step1", step1)
workflow.add_node("step2", step2)
workflow.add_node("step3", step3)

workflow.add_edge(START, "step1")
workflow.add_edge("step1", "step2")
workflow.add_edge("step2", "step3")
workflow.add_edge("step3", END)

# åœ¨ç‰¹å®šèŠ‚ç‚¹å‰åè®¾ç½®æ–­ç‚¹
app = workflow.compile(
    checkpointer=InMemorySaver(),
    interrupt_before=["step2"],  # åœ¨ step2 ä¹‹å‰æš‚åœ
    interrupt_after=["step3"]    # åœ¨ step3 ä¹‹åæš‚åœ
)

config = {"configurable": {"thread_id": "debug_1"}}

# è¿è¡Œåˆ°ç¬¬ä¸€ä¸ªæ–­ç‚¹
result1 = app.invoke({"data": "test"}, config)
print(f"æ‰§è¡Œåˆ°: step{result1.get('step', 0)}")  # step1

# ç»§ç»­æ‰§è¡Œåˆ°ä¸‹ä¸€ä¸ªæ–­ç‚¹
result2 = app.invoke(None, config)  # ä¼ å…¥ None ç»§ç»­
print(f"æ‰§è¡Œåˆ°: step{result2.get('step', 0)}")  # step2

# ç»§ç»­æ‰§è¡Œåˆ°ç»“æŸ
result3 = app.invoke(None, config)
print(f"æ‰§è¡Œåˆ°: step{result3.get('step', 0)}")  # step3
```

---

## ç¬¬å…­éƒ¨åˆ†ï¼šå®Œæ•´å®æˆ˜é¡¹ç›®

### 6.1 é¡¹ç›®ä¸€ï¼šæ™ºèƒ½æ–‡æ¡£é—®ç­”ç³»ç»Ÿ

è¿™æ˜¯ä¸€ä¸ªå®Œæ•´çš„RAGæ–‡æ¡£é—®ç­”ç³»ç»Ÿï¼ŒåŒ…å«æ–‡æ¡£ç®¡ç†ã€æ£€ç´¢å’Œé—®ç­”åŠŸèƒ½ã€‚

```python
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_chroma import Chroma
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.documents import Document
from langchain.tools import tool
from langchain.agents import create_agent
from langgraph.checkpoint.memory import InMemorySaver
from typing import List, Dict
from pathlib import Path

class DocumentQASystem:
    """æ™ºèƒ½æ–‡æ¡£é—®ç­”ç³»ç»Ÿ"""
    
    def __init__(self, persist_directory: str = "./doc_qa_db"):
        self.persist_directory = persist_directory
        self.embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
        self.vector_store = None
        self.agent = None
        self._setup()
    
    def _setup(self):
        """åˆå§‹åŒ–ç³»ç»Ÿ"""
        # åˆ›å»ºå‘é‡å­˜å‚¨
        self.vector_store = Chroma(
            collection_name="documents",
            embedding_function=self.embeddings,
            persist_directory=self.persist_directory
        )
        
        # åˆ›å»ºæ£€ç´¢å·¥å…·
        @tool(response_format="content_and_artifact")
        def retrieve_documents(query: str, top_k: int = 3) -> tuple[str, List[Document]]:
            """ä»çŸ¥è¯†åº“æ£€ç´¢ç›¸å…³æ–‡æ¡£
            
            Args:
                query: æœç´¢æŸ¥è¯¢
                top_k: è¿”å›å‰Kä¸ªç»“æœ
            """
            docs = self.vector_store.similarity_search(query, k=top_k)
            
            formatted = f"æ£€ç´¢åˆ° {len(docs)} ä¸ªç›¸å…³æ–‡æ¡£ï¼š\n\n"
            for i, doc in enumerate(docs, 1):
                source = doc.metadata.get("source", "unknown")
                formatted += f"ã€æ–‡æ¡£ {i}ã€‘æ¥æº: {source}\n"
                formatted += f"{doc.page_content}\n\n"
            
            return formatted, docs
        
        # åˆ›å»ºAgent
        self.agent = create_agent(
            model="gpt-4",
            tools=[retrieve_documents],
            checkpointer=InMemorySaver(),
            system_prompt="""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æ¡£é—®ç­”åŠ©æ‰‹ã€‚

å·¥ä½œæµç¨‹ï¼š
1. ä½¿ç”¨ retrieve_documents æ£€ç´¢ç›¸å…³æ–‡æ¡£
2. ä»”ç»†åˆ†ææ–‡æ¡£å†…å®¹
3. åŸºäºæ–‡æ¡£æä¾›å‡†ç¡®ç­”æ¡ˆ
4. å¼•ç”¨å…·ä½“çš„æ–‡æ¡£æ¥æº

åŸåˆ™ï¼š
- å§‹ç»ˆåŸºäºæ£€ç´¢åˆ°çš„æ–‡æ¡£å›ç­”
- å¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰ç­”æ¡ˆï¼Œè¯šå®è¯´æ˜
- æä¾›è¯¦ç»†ä¸”æœ‰å¸®åŠ©çš„å›ç­”
- å¼•ç”¨å…·ä½“æ¥æº
"""
        )
    
    def add_documents(self, documents: List[Dict[str, str]]):
        """æ·»åŠ æ–‡æ¡£åˆ°çŸ¥è¯†åº“
        
        Args:
            documents: æ–‡æ¡£åˆ—è¡¨ï¼Œæ ¼å¼ [{"content": "...", "metadata": {...}}]
        """
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200
        )
        
        all_splits = []
        for doc in documents:
            content = doc["content"]
            metadata = doc.get("metadata", {})
            
            splits = text_splitter.split_text(content)
            for split in splits:
                all_splits.append(
                    Document(
                        page_content=split,
                        metadata=metadata
                    )
                )
        
        self.vector_store.add_documents(all_splits)
        print(f"âœ… å·²æ·»åŠ  {len(all_splits)} ä¸ªæ–‡æ¡£å—")
    
    def ask(self, question: str, thread_id: str = "default") -> str:
        """æé—®
        
        Args:
            question: é—®é¢˜
            thread_id: ä¼šè¯IDï¼ˆç”¨äºå¤šè½®å¯¹è¯ï¼‰
        
        Returns:
            å›ç­”
        """
        config = {"configurable": {"thread_id": thread_id}}
        
        result = self.agent.invoke(
            {"messages": [{"role": "user", "content": question}]},
            config
        )
        
        return result["messages"][-1].content
    
    def clear_history(self, thread_id: str = "default"):
        """æ¸…é™¤å¯¹è¯å†å²"""
        # åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿™é‡Œåº”è¯¥æ¸…é™¤ checkpointer ä¸­çš„å†å²
        pass


# === ä½¿ç”¨ç¤ºä¾‹ ===

# 1. åˆ›å»ºç³»ç»Ÿ
qa_system = DocumentQASystem()

# 2. æ·»åŠ æ–‡æ¡£
documents = [
    {
        "content": """
        LangChain ç®€ä»‹
        
        LangChain æ˜¯ä¸€ä¸ªç”¨äºå¼€å‘ç”±è¯­è¨€æ¨¡å‹é©±åŠ¨çš„åº”ç”¨ç¨‹åºçš„æ¡†æ¶ã€‚
        å®ƒæä¾›äº†æ ‡å‡†åŒ–çš„æ¥å£æ¥é›†æˆä¸åŒçš„è¯­è¨€æ¨¡å‹ï¼Œ
        ä»¥åŠç”¨äºæ„å»ºå¤æ‚åº”ç”¨çš„å·¥å…·å’Œç»„ä»¶ã€‚
        
        æ ¸å¿ƒç»„ä»¶åŒ…æ‹¬ï¼š
        - Models: è¯­è¨€æ¨¡å‹é›†æˆ
        - Prompts: æç¤ºæ¨¡æ¿ç®¡ç†
        - Chains: ç»„ä»¶ç¼–æ’
        - Agents: æ™ºèƒ½å†³ç­–ç³»ç»Ÿ
        - Memory: å¯¹è¯è®°å¿†
        """,
        "metadata": {"source": "langchain_intro.md", "category": "åŸºç¡€"}
    },
    {
        "content": """
        LangGraph å·¥ä½œæµ
        
        LangGraph æ˜¯ LangChain çš„æ‰©å±•ï¼Œç”¨äºæ„å»ºæœ‰çŠ¶æ€çš„å¤šæ­¥éª¤åº”ç”¨ã€‚
        å®ƒåŸºäºå›¾çš„æ¦‚å¿µï¼Œé€šè¿‡å®šä¹‰èŠ‚ç‚¹å’Œè¾¹æ¥åˆ›å»ºå¤æ‚çš„å·¥ä½œæµã€‚
        
        ä¸»è¦ç‰¹æ€§ï¼š
        - StateGraph: çŠ¶æ€å›¾ç®¡ç†
        - æ¡ä»¶è·¯ç”±: åŠ¨æ€å†³ç­–
        - å¹¶è¡Œæ‰§è¡Œ: æé«˜æ•ˆç‡
        - Checkpointer: çŠ¶æ€æŒä¹…åŒ–
        
        é€‚ç”¨åœºæ™¯ï¼š
        - å¤æ‚çš„å¤šæ­¥éª¤æ¨ç†
        - éœ€è¦ç²¾ç¡®æ§åˆ¶çš„å·¥ä½œæµ
        - å¤šAgentåä½œç³»ç»Ÿ
        """,
        "metadata": {"source": "langgraph_guide.md", "category": "è¿›é˜¶"}
    },
    {
        "content": """
        RAG æœ€ä½³å®è·µ
        
        æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰çš„ä¼˜åŒ–æŠ€å·§ï¼š
        
        1. æ–‡æ¡£åˆ†å‰²ç­–ç•¥
           - æ ¹æ®å†…å®¹ç»“æ„åˆ†å‰²
           - ä¿æŒè¯­ä¹‰å®Œæ•´æ€§
           - è®¾ç½®åˆç†çš„é‡å 
        
        2. æ£€ç´¢ä¼˜åŒ–
           - æ··åˆæœç´¢ï¼ˆå…³é”®è¯+å‘é‡ï¼‰
           - æŸ¥è¯¢æ‰©å±•
           - ç»“æœé‡æ’åº
        
        3. æç¤ºå·¥ç¨‹
           - æ˜ç¡®æŒ‡ç¤ºä½¿ç”¨æ£€ç´¢å†…å®¹
           - è¦æ±‚å¼•ç”¨æ¥æº
           - å¤„ç†æ— ç­”æ¡ˆæƒ…å†µ
        """,
        "metadata": {"source": "rag_best_practices.md", "category": "æŠ€å·§"}
    }
]

qa_system.add_documents(documents)

# 3. æé—®
questions = [
    "ä»€ä¹ˆæ˜¯ LangChainï¼Ÿ",
    "LangGraph æœ‰å“ªäº›ç‰¹æ€§ï¼Ÿ",
    "RAG æœ‰ä»€ä¹ˆä¼˜åŒ–æŠ€å·§ï¼Ÿ",
    "å¦‚ä½•éƒ¨ç½² LangChain åº”ç”¨ï¼Ÿ"  # çŸ¥è¯†åº“ä¸­æ²¡æœ‰
]

for question in questions:
    print(f"\n{'='*60}")
    print(f"é—®é¢˜: {question}")
    print('='*60)
    answer = qa_system.ask(question)
    print(f"\nå›ç­”:\n{answer}")
```

### 6.2 é¡¹ç›®æ€»ç»“

æ­å–œä½ å®Œæˆç¬¬ä¸‰é˜¶æ®µçš„å­¦ä¹ ï¼ä½ ç°åœ¨å·²ç»æŒæ¡äº†ï¼š

âœ… **Tools å·¥å…·ç³»ç»Ÿ**
- å¤æ‚å‚æ•°å®šä¹‰
- è¿”å›å€¼æ¨¡å¼
- è¿è¡Œæ—¶ä¿¡æ¯è®¿é—®
- å·¥å…·ç»„åˆç­–ç•¥

âœ… **RAG æ£€ç´¢å¢å¼ºç”Ÿæˆ**
- å‘é‡å­˜å‚¨ä½¿ç”¨
- æ–‡æ¡£åŠ è½½åˆ†å‰²
- RAG ç³»ç»Ÿæ„å»º
- ä¼˜åŒ–æŠ€å·§

âœ… **Streaming æµå¼å¤„ç†**
- å¤šç§æµå¼æ¨¡å¼
- è‡ªå®šä¹‰äº‹ä»¶
- æµå¼èŠå¤©åº”ç”¨

âœ… **Structured Output**
- Pydantic æ¨¡å‹
- ToolStrategy
- éªŒè¯å’Œé”™è¯¯å¤„ç†

âœ… **Middleware ä¸ä¸­æ–­**
- äººå·¥ä»‹å…¥å¾ªç¯
- Interrupt æœºåˆ¶
- å®¡æ‰¹å·¥ä½œæµ

âœ… **å®Œæ•´å®æˆ˜é¡¹ç›®**
- æ–‡æ¡£é—®ç­”ç³»ç»Ÿ
- ç”Ÿäº§çº§ä»£ç 

### ğŸ“Œ ç»§ç»­å­¦ä¹ 

æ¥ä¸‹æ¥ä½ å¯ä»¥ï¼š
1. ğŸš€ æ·±å…¥ç ”ç©¶ LangSmithï¼ˆç›‘æ§å’Œè¿½è¸ªï¼‰
2. ğŸŒ å­¦ä¹  LangServeï¼ˆéƒ¨ç½²æœåŠ¡ï¼‰
3. ğŸ—ï¸  æ„å»ºè‡ªå·±çš„ AI åº”ç”¨
4. ğŸ“– é˜…è¯»å®˜æ–¹æ–‡æ¡£æ·±å…¥å­¦ä¹ 

æ­å–œä½ å®Œæˆäº† LangChain å®Œæ•´å­¦ä¹ è®¡åˆ’ï¼ğŸ‰

